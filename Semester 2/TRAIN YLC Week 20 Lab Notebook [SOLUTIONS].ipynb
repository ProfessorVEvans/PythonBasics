{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1PupwQQp6LZFfMIPgWLKlWXrYDgLuA5sj","timestamp":1678127175525}],"collapsed_sections":["7dzC09dLlEhm"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["# **Lab 20: Natural Language Processing I**\n","---\n","\n","### **Description**\n","In this week's lab, we will see how to use neural networks for one of the most popular NLP tasks: **text classification**. This will involve applying what you already know about neural nets and new NLP concepts of tokenization and vectorization.\n","\n","For this project, we will be working with the `fetch_20newsgroups` dataset, which is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups. Each newsgroup covers a different topic, such as sports, politics, religion, and technology. The documents within each newsgroup were posted by various authors, and cover a wide range of subtopics related to the main theme of the newsgroup.\n","\n","The goal of this project is to build a machine learning model that can accurately classify newsgroup documents based on their content.\n","\n","<br>\n","\n","### **Lab Structure**\n","**Part 1**: [News Subject Classification with a DNN](#p1)\n","\n","**Part 2**: [News Subject Classification with a CNN](#p2)\n","\n","\n","\n","\n","\n","<br>\n","\n","### **Goals**\n","By the end of this lab, you will:\n","* Understand the concept of tokenization in NLP.\n","* Compare a fully connected network to a CNN for text classification.\n","\n","<br>\n","\n","### **Cheat Sheets**\n","[Natural Language Processing I](https://docs.google.com/document/d/1ZaLtMF7aQsG05myetJpoTJlr-sAIURP_a9sQr66pfqw/edit?usp=drive_link)\n","\n","<br>\n","\n","**Before starting, run the code below to import all necessary functions and libraries.**\n"],"metadata":{"id":"mbZXQ3rA3NwL"}},{"cell_type":"code","source":["!pip install --quiet torch==1.13.1\n","!pip install --quiet torchdata==0.5.1\n","\n","!pip install torch\n","!pip install torchtext\n","import torchtext\n","\n","import numpy as np\n","import pandas as pd\n","\n","import tensorflow as tf\n","from keras.models import Sequential\n","from keras.layers import *\n","from keras.optimizers import Adam\n","from keras.utils import to_categorical\n","\n","from sklearn.model_selection import train_test_split\n","\n","import warnings\n","warnings.filterwarnings('ignore')"],"metadata":{"id":"YAvvLhRIoqYp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1704341005897,"user_tz":480,"elapsed":135978,"user":{"displayName":"Angelique Membrido","userId":"04984385250682432912"}},"outputId":"50e21fa5-9f16-4be4-852f-400af291905e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchaudio 2.1.0+cu121 requires torch==2.1.0, but you have torch 1.13.1 which is incompatible.\n","torchdata 0.7.0 requires torch==2.1.0, but you have torch 1.13.1 which is incompatible.\n","torchtext 0.16.0 requires torch==2.1.0, but you have torch 1.13.1 which is incompatible.\n","torchvision 0.16.0+cu121 requires torch==2.1.0, but you have torch 1.13.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchtext 0.16.0 requires torch==2.1.0, but you have torch 1.13.1 which is incompatible.\n","torchtext 0.16.0 requires torchdata==0.7.0, but you have torchdata 0.5.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n","\u001b[0mRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (1.13.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n","Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch) (11.7.99)\n","Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/dist-packages (from torch) (8.5.0.96)\n","Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/dist-packages (from torch) (11.10.3.66)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch) (11.7.99)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (67.7.2)\n","Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.42.0)\n","\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n","\u001b[0mRequirement already satisfied: torchtext in /usr/local/lib/python3.10/dist-packages (0.16.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext) (4.66.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext) (2.31.0)\n","Collecting torch==2.1.0 (from torchtext)\n","  Using cached torch-2.1.0-cp310-cp310-manylinux1_x86_64.whl (670.2 MB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext) (1.23.5)\n","Collecting torchdata==0.7.0 (from torchtext)\n","  Using cached torchdata-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchtext) (3.13.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchtext) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchtext) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchtext) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchtext) (3.1.2)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchtext) (2023.6.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchtext) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchtext) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchtext) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchtext) (8.9.2.26)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchtext) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchtext) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchtext) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchtext) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchtext) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchtext) (2.18.1)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchtext) (12.1.105)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchtext) (2.1.0)\n","Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata==0.7.0->torchtext) (2.0.7)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.1.0->torchtext) (12.3.101)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.6)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (2023.11.17)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.1.0->torchtext) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.1.0->torchtext) (1.3.0)\n","\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n","\u001b[0mInstalling collected packages: torch, torchdata\n","  Attempting uninstall: torch\n","    Found existing installation: torch 1.13.1\n","    Uninstalling torch-1.13.1:\n","      Successfully uninstalled torch-1.13.1\n","  Attempting uninstall: torchdata\n","    Found existing installation: torchdata 0.5.1\n","    Uninstalling torchdata-0.5.1:\n","      Successfully uninstalled torchdata-0.5.1\n","Successfully installed torch-2.1.0 torchdata-0.7.0\n"]}]},{"cell_type":"markdown","source":["---\n","## **Part 1: News Subject Classification with a DNN**\n","---\n","\n","In this section, we will learn how to tokenize the results of a neural network classifying news articles by subject. We will see how to use keras's `TextVectorization` layer alongside the other layers we have seen to classify these articles.\n","\n","We will be working with the [AG News dataset](http://groups.di.unipi.it/~gulli/AG_corpus_of_news_articles.html), which is a corpus of over 1 million articles from more than 2000 sources and is commonly used in academic research (see its [papers with code page](https://paperswithcode.com/dataset/ag-news) for more information).\n","\n","<br>\n","\n","Each input will be text containing an article's title, source, and a snippet from the article itself. We will then classify each input as one of these subjects: `\"World\"`, `\"Sports\"`, `\"Business\"`, or `\"Sci/Tech\"`.\n","\n","<br>\n","\n","\n","**Run the code provided below to import the dataset.**"],"metadata":{"id":"idga37M2FsMR"}},{"cell_type":"code","source":["train_dataset, test_dataset = torchtext.datasets.AG_NEWS()\n","\n","x_train, y_train = [], []\n","for Y, X in train_dataset:\n","    x_train.append(X)\n","    y_train.append(Y)\n","\n","x_test, y_test = [], []\n","for Y, X in test_dataset:\n","    x_test.append(X)\n","    y_test.append(Y)\n","\n","x_train = np.array(x_train)\n","x_test = np.array(x_test)\n","\n","y_train, y_test = np.array(y_train) - 1, np.array(y_test) - 1\n","y_train = to_categorical(y_train, dtype = 'int32')\n","y_test = to_categorical(y_test, dtype = 'int32')"],"metadata":{"id":"In9BSfqd_OPA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Problem #1.1: Create the `TextVectorization` layer**\n","\n","Let's create a `TextVectorization` layer to vectorize this data.\n","\n","Specifically,\n","1. Initialize the layer with the specified parameters.\n","\n","2. Adapt the layer to the training data."],"metadata":{"id":"E90o2LJcwsMI"}},{"cell_type":"markdown","source":["##### **1. Initialize the layer with the specified parameters.**\n","\n","* `max_tokens = 5000`\n","* `output_mode = 'int'`\n","* `output_sequence_length = 50`"],"metadata":{"id":"koGBxiapJC_y"}},{"cell_type":"code","source":["vectorize_layer = TextVectorization(\n","    # WRITE YOUR CODE HERE\n","  )"],"metadata":{"id":"sXTATRLVm2Mm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###### **Solution**\n"],"metadata":{"id":"53S06AUNm2Mm"}},{"cell_type":"code","source":["vectorize_layer = TextVectorization(\n","    max_tokens = 5000,\n","    output_mode = 'int',\n","    output_sequence_length = 50\n","  )\n","\n","vectorize_layer.adapt(x_train)"],"metadata":{"id":"f6TmxPBSm2Mm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### ***STOP!* Answer the following question: Why does every output sequence need to be the same length?**"],"metadata":{"id":"qMFXTuDwvJxv"}},{"cell_type":"markdown","source":["##### **2. Adapt the layer to the training data.**"],"metadata":{"id":"wHH-kKo9pi7k"}},{"cell_type":"code","source":[],"metadata":{"id":"36mXm5tkpwLx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###### **Solution**"],"metadata":{"id":"bOHPi8Nlpy7L"}},{"cell_type":"code","source":["vectorize_layer.adapt(x_train)"],"metadata":{"id":"DFmEnApbpxUg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Problem #1.2: Look at the vocabulary**\n","\n","\n","**Run the code below to look at a portion of the vocabulary that was just built for the training data.**"],"metadata":{"id":"Vz2dKD8pJhXa"}},{"cell_type":"code","source":["vectorize_layer.get_vocabulary()[0:50]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pA6CEFVtDeQa","executionInfo":{"status":"ok","timestamp":1704341099816,"user_tz":480,"elapsed":144,"user":{"displayName":"Angelique Membrido","userId":"04984385250682432912"}},"outputId":"419ef6e9-90a5-4f5c-da99-7910c6f04c02"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['',\n"," '[UNK]',\n"," 'the',\n"," 'to',\n"," 'a',\n"," 'of',\n"," 'in',\n"," 'and',\n"," 'on',\n"," 'for',\n"," '39s',\n"," 'that',\n"," 'with',\n"," 'as',\n"," 'at',\n"," 'its',\n"," 'is',\n"," 'new',\n"," 'by',\n"," 'said',\n"," 'it',\n"," 'us',\n"," 'has',\n"," 'from',\n"," 'reuters',\n"," 'an',\n"," 'ap',\n"," 'his',\n"," 'will',\n"," 'after',\n"," 'was',\n"," 'be',\n"," 'over',\n"," 'have',\n"," 'their',\n"," 'are',\n"," 'up',\n"," 'but',\n"," 'first',\n"," 'more',\n"," 'two',\n"," 'he',\n"," 'this',\n"," 'world',\n"," 'monday',\n"," 'wednesday',\n"," 'tuesday',\n"," 'oil',\n"," 'out',\n"," 'thursday']"]},"metadata":{},"execution_count":4}]},{"cell_type":"markdown","source":["#### **Problem #1.3: Add the input and text vectorization layers to the model**\n","\n","\n"],"metadata":{"id":"zTCj-Z7rJ5on"}},{"cell_type":"code","source":["model = Sequential()\n","\n","model.add(Input(# COMPLETE THIS LINE\n","model.add(# COMPLETE THIS LINE"],"metadata":{"id":"RYh5A78OKRhm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###### **Solution**\n"],"metadata":{"id":"qnyPW50LKRE0"}},{"cell_type":"code","source":["model = Sequential()\n","\n","model.add(Input(shape=(1,), dtype=tf.string))\n","model.add(vectorize_layer)"],"metadata":{"id":"0qIl7LcrD8B3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Problem #1.4: Look at the vectorization of an example**\n","\n","\n","Add your own sentence below to see how it would be vectorized with our newly adapted layer.\n","\n","<br>\n","\n","**NOTE:** `TextVectorizer` will ignore any punctuation and consider upper and lower case the same. There are extra parameters that can set to adjust this."],"metadata":{"id":"Ohs_ludzKcE9"}},{"cell_type":"code","source":["vector_0 = model.predict([# COMPLETE THIS LINE\n","\n","print(vector_0)\n","print(vector_0.shape)"],"metadata":{"id":"PZoit5S6OnPj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###### **Solution**\n"],"metadata":{"id":"FGHYd5ODOmkh"}},{"cell_type":"code","source":["vector_0 = model.predict(['hello world my name is Adam'])\n","\n","print(vector_0)\n","print(vector_0.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f_RHvezKFPhN","executionInfo":{"status":"ok","timestamp":1704341325417,"user_tz":480,"elapsed":433,"user":{"displayName":"Angelique Membrido","userId":"04984385250682432912"}},"outputId":"28fc8b2a-0f07-4d48-8dfe-ad33f8d7bee5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 0s 185ms/step\n","[[   1   43 1293  924   16 4087    0    0    0    0    0    0    0    0\n","     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","     0    0    0    0    0    0    0    0]]\n","(1, 50)\n"]}]},{"cell_type":"markdown","source":["#### ***STOP!* Answer the following question: What does each number in the output vector represent?**"],"metadata":{"id":"iNEGQr1YumaR"}},{"cell_type":"markdown","source":["#### **Problem #1.5: Add hidden layers and an output layer**\n","\n","\n","Add two dense layers with 512 neurons and ReLU activation.\n","\n","Then, create the output layer so we can classify the data as `\"World\"`, `\"Sports\"`, `\"Business\"`, or `\"Sci/Tech\"`. You will use the softmax activation function."],"metadata":{"id":"RbqWrZRxm2Mn"}},{"cell_type":"code","source":["model.add(Dense(# COMPLETE THIS LINE\n","model.add(Dense(# COMPLETE THIS LINE\n","model.add(Dense(# COMPLETE THIS LINE"],"metadata":{"id":"c3yMuWf1m2Mn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###### **Solution**\n"],"metadata":{"id":"wWzcz2l8m2Mn"}},{"cell_type":"code","source":["model.add(Dense(512, activation = 'relu'))\n","model.add(Dense(512, activation = 'relu'))\n","model.add(Dense(4, activation = 'softmax'))"],"metadata":{"id":"bE1a59pUm2Mn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's take a look at our DNN."],"metadata":{"id":"zYru7HAFGz3B"}},{"cell_type":"code","source":["model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VI8Yhy61G2Ar","executionInfo":{"status":"ok","timestamp":1704341336272,"user_tz":480,"elapsed":119,"user":{"displayName":"Angelique Membrido","userId":"04984385250682432912"}},"outputId":"21d4cccb-c9eb-4be0-c9f0-5113a852d898"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," text_vectorization (TextVe  (None, 50)                0         \n"," ctorization)                                                    \n","                                                                 \n"," dense (Dense)               (None, 512)               26112     \n","                                                                 \n"," dense_1 (Dense)             (None, 512)               262656    \n","                                                                 \n"," dense_2 (Dense)             (None, 4)                 2052      \n","                                                                 \n","=================================================================\n","Total params: 290820 (1.11 MB)\n","Trainable params: 290820 (1.11 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]}]},{"cell_type":"markdown","source":["#### **Problem #1.6: Compile and fit the model**\n","\n","\n","Compile and fit the model with the following parameters:\n","* Adam learning rate of 0.001\n","* `categorical_crossentropy` for the loss function\n","* Accuracy as the metric\n","* For the fit, use `epochs=5` and `batch_size=256`"],"metadata":{"id":"EI8BNJxFm2Mn"}},{"cell_type":"code","source":["opt = Adam(learning_rate = # COMPLETE THIS LINE\n","model.compile(optimizer = opt, loss = # COMPLETE THIS LINE\n","\n","model.fit(# COMPLETE THIS LINE"],"metadata":{"id":"rauKiC4ym2Mn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###### **Solution**\n"],"metadata":{"id":"7zNuZGgUm2Mn"}},{"cell_type":"code","source":["opt = Adam(learning_rate = 0.001)\n","model.compile(optimizer = opt, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n","\n","model.fit(x_train, y_train, epochs = 5, batch_size = 256)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1704341391405,"user_tz":480,"elapsed":49143,"user":{"displayName":"Angelique Membrido","userId":"04984385250682432912"}},"outputId":"692dfe0f-beb3-45e5-9623-7092888751ab","id":"Y_Inzwwkm2Mn"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/5\n","469/469 [==============================] - 11s 20ms/step - loss: 26.7041 - accuracy: 0.2583\n","Epoch 2/5\n","469/469 [==============================] - 9s 20ms/step - loss: 1.4031 - accuracy: 0.2528\n","Epoch 3/5\n","469/469 [==============================] - 10s 20ms/step - loss: 1.3862 - accuracy: 0.2554\n","Epoch 4/5\n","469/469 [==============================] - 9s 19ms/step - loss: 1.3835 - accuracy: 0.2556\n","Epoch 5/5\n","469/469 [==============================] - 9s 20ms/step - loss: 1.3826 - accuracy: 0.2524\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.src.callbacks.History at 0x7ed767bb5ea0>"]},"metadata":{},"execution_count":11}]},{"cell_type":"markdown","source":["#### **Problem #1.7: Evaluate the model**\n","\n","\n","Now, evaluate the model for both the training and test sets.\n","\n","<br>\n","\n","**NOTE:** As a baseline, randomly guessing 1 out of 4 possible classes would achieve a roughly 0.25 accuracy."],"metadata":{"id":"9Brgtf8cm2Mn"}},{"cell_type":"code","source":["# Evaluate the training set\n","model.evaluate(# COMPLETE THIS LINE\n","\n","# Evaluate the test set\n","model.evaluate(# COMPLETE THIS LINE"],"metadata":{"id":"VpcaQMrfm2Mn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###### **Solution**\n"],"metadata":{"id":"cDAMiAtSm2Mn"}},{"cell_type":"code","source":["model.evaluate(x_train, y_train)\n","model.evaluate(x_test, y_test)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1704341911277,"user_tz":480,"elapsed":21087,"user":{"displayName":"Angelique Membrido","userId":"04984385250682432912"}},"outputId":"ec997faf-e0dc-419d-9d56-f1f487676025","id":"IksyCOG7m2Mn"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["3750/3750 [==============================] - 17s 4ms/step - loss: 1.3813 - accuracy: 0.2570\n","238/238 [==============================] - 2s 5ms/step - loss: 1.4008 - accuracy: 0.2528\n"]},{"output_type":"execute_result","data":{"text/plain":["[1.4007633924484253, 0.25276315212249756]"]},"metadata":{},"execution_count":12}]},{"cell_type":"markdown","source":["---\n","\n","<center>\n","\n","### **Back to lecture**\n","\n","---"],"metadata":{"id":"9l2YAKbUv7JW"}},{"cell_type":"markdown","source":["---\n","## **Part 2: News Subject Classification with a CNN**\n","---\n","\n","The model in Part 1 likely did not do much better than random guessing. Let's try with a CNN instead."],"metadata":{"id":"q-mvo-kmm2Mp"}},{"cell_type":"markdown","source":["Let's start by building a new CNN model. Remember, the syntax for CNNs for NLP is a little different than for images. We will be using the 1D versions of the convolution and max pooling layers. Examples:\n","* `Conv1D(filters=128, kernel_size=5, activation='relu')`\n","* `MaxPooling1D(pool_size=2)`\n"],"metadata":{"id":"gyAN7zkgpRRZ"}},{"cell_type":"markdown","source":["#### **Problem #2.1: Initialize the model with an input and vectorizer layer**\n","\n","\n","*Hint: This is the same as last time.*"],"metadata":{"id":"VN8Z0gy_sExR"}},{"cell_type":"code","source":["cnn_model = # COMPLETE THIS LINE\n","\n","cnn_model.add(Input(# COMPLETE THIS LINE\n","cnn_model.add(# COMPLETE THIS LINE"],"metadata":{"id":"LvS3JpkcsZI2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###### **Solution**\n"],"metadata":{"id":"exLTTU5JtntN"}},{"cell_type":"code","source":["cnn_model = Sequential()\n","\n","cnn_model.add(Input(shape=(1,), dtype=tf.string))\n","cnn_model.add(vectorize_layer)"],"metadata":{"id":"z6DOyy7etntN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Problem #2.2: Finish building the CNN**\n","\n","\n","Build your CNN with the following layers:\n","* a convolutional layer with 64 filters, a kernel size of 5, and ReLU activation\n","* a max pooling layer with a pool size of 2\n","* a convolutional layer with 128 filters, a kernel size of 5, and ReLU activation\n","* a max pooling layer with a pool size of 2\n","* a flatten layer\n","* a dense layer with 256 neurons\n","* the output layer"],"metadata":{"id":"9kV4xruit_UF"}},{"cell_type":"code","source":["# The convolution layer requires us to cast the inputs to a different data type\n","# and reshape the input as well. We have done this for you.\n","cnn_model.add(Lambda(lambda x: tf.cast(x, 'float32')))\n","cnn_model.add(Reshape((50, 1)))\n","\n","# Start building your CNN below.\n","# WRITE YOUR CODE HERE"],"metadata":{"id":"8SQd40rbnOrq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###### **Solution**"],"metadata":{"id":"b6IWnBGBnPbi"}},{"cell_type":"code","source":["# The convolution layer requires us to cast the inputs to a different data type\n","# and reshape the input as well. We have done this for you.\n","cnn_model.add(Lambda(lambda x: tf.cast(x, 'float32')))\n","cnn_model.add(Reshape((50, 1)))\n","\n","# Start building your CNN below.\n","cnn_model.add(Conv1D(filters=64, kernel_size=5, activation='relu'))\n","cnn_model.add(MaxPooling1D(pool_size=2))\n","cnn_model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n","cnn_model.add(MaxPooling1D(pool_size=2))\n","cnn_model.add(Flatten())\n","cnn_model.add(Dense(256, activation = 'relu'))\n","cnn_model.add(Dense(4, activation = 'softmax'))"],"metadata":{"id":"QYEmDsS1ucWW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's take a look at the completed model."],"metadata":{"id":"VfFAaOAOizPy"}},{"cell_type":"code","source":["cnn_model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lqK1H52pi1iZ","executionInfo":{"status":"ok","timestamp":1704343618476,"user_tz":480,"elapsed":274,"user":{"displayName":"Angelique Membrido","userId":"04984385250682432912"}},"outputId":"c51fe2b1-968f-4d49-9ef6-8a2e8a2e4150"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_2\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," text_vectorization (TextVe  (None, 50)                0         \n"," ctorization)                                                    \n","                                                                 \n"," lambda (Lambda)             (None, 50)                0         \n","                                                                 \n"," reshape (Reshape)           (None, 50, 1)             0         \n","                                                                 \n"," conv1d (Conv1D)             (None, 46, 64)            384       \n","                                                                 \n"," max_pooling1d (MaxPooling1  (None, 23, 64)            0         \n"," D)                                                              \n","                                                                 \n"," conv1d_1 (Conv1D)           (None, 19, 128)           41088     \n","                                                                 \n"," max_pooling1d_1 (MaxPoolin  (None, 9, 128)            0         \n"," g1D)                                                            \n","                                                                 \n"," flatten (Flatten)           (None, 1152)              0         \n","                                                                 \n"," dense_3 (Dense)             (None, 256)               295168    \n","                                                                 \n"," dense_4 (Dense)             (None, 4)                 1028      \n","                                                                 \n","=================================================================\n","Total params: 337668 (1.29 MB)\n","Trainable params: 337668 (1.29 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]}]},{"cell_type":"markdown","source":["#### **Problem #2.3: Compile and fit the model**\n","\n","\n","Compile and fit the model with the same parameters as Part 1."],"metadata":{"id":"kRhmTL_2lOM9"}},{"cell_type":"code","source":["opt = Adam(# COMPLETE THIS LINE)\n","cnn_model.compile(# COMPLETE THIS LINE\n","\n","cnn_model.fit(# COMPLETE THIS LINE"],"metadata":{"id":"RKS-Rxcsla4f"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###### **Solution**\n"],"metadata":{"id":"ZTyBqTsIlaW_"}},{"cell_type":"code","source":["opt = Adam(learning_rate = 0.001)\n","cnn_model.compile(optimizer = opt, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n","\n","cnn_model.fit(x_train, y_train, epochs = 5, batch_size = 256)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uGe4e4t4i9-9","executionInfo":{"status":"ok","timestamp":1704343786788,"user_tz":480,"elapsed":162876,"user":{"displayName":"Angelique Membrido","userId":"04984385250682432912"}},"outputId":"fb952712-9a38-4bb9-faeb-5d633c84f2f1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/5\n","469/469 [==============================] - 39s 80ms/step - loss: 6.0834 - accuracy: 0.2771\n","Epoch 2/5\n","469/469 [==============================] - 30s 65ms/step - loss: 1.3744 - accuracy: 0.2911\n","Epoch 3/5\n","469/469 [==============================] - 31s 67ms/step - loss: 1.3732 - accuracy: 0.2925\n","Epoch 4/5\n","469/469 [==============================] - 30s 64ms/step - loss: 1.3731 - accuracy: 0.2934\n","Epoch 5/5\n","469/469 [==============================] - 30s 64ms/step - loss: 1.3734 - accuracy: 0.2900\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.src.callbacks.History at 0x7ed767885030>"]},"metadata":{},"execution_count":16}]},{"cell_type":"markdown","source":["#### **Problem #2.4: Evaluate the model**\n","\n","\n","Now, evaluate the model for both the training and test sets."],"metadata":{"id":"FTIEF_GHlGZA"}},{"cell_type":"code","source":["cnn_model.evaluate(# COMPLETE THIS LINE\n","cnn_model.evaluate(# COMPLETE THIS LINE"],"metadata":{"id":"1VJ42ZnfloOQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###### **Solution**"],"metadata":{"id":"99PwvRwPllgT"}},{"cell_type":"code","source":["cnn_model.evaluate(x_train, y_train)\n","cnn_model.evaluate(x_test, y_test)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qji3x9bcj2SF","executionInfo":{"status":"ok","timestamp":1704343835384,"user_tz":480,"elapsed":45578,"user":{"displayName":"Angelique Membrido","userId":"04984385250682432912"}},"outputId":"835fdf98-197f-4f37-935f-610f225c1327"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["3750/3750 [==============================] - 25s 7ms/step - loss: 1.3736 - accuracy: 0.2899\n","238/238 [==============================] - 2s 6ms/step - loss: 1.3738 - accuracy: 0.2884\n"]},{"output_type":"execute_result","data":{"text/plain":["[1.373782753944397, 0.28842106461524963]"]},"metadata":{},"execution_count":17}]},{"cell_type":"markdown","source":["**Oh no!** It looks like the CNN didn't do much better! It turns out that tokenization and vectorization is not enough to prepare text data for deep learning. There's an additional processing step we can take that will set our models up for success: **embedding.** We will see how embedding improves model performance in next week's lab."],"metadata":{"id":"CGszeqCvm2Mt"}},{"cell_type":"markdown","source":["# End of notebook\n","---\n","Â© 2024 The Coding School, All rights reserved"],"metadata":{"id":"7dzC09dLlEhm"}}]}