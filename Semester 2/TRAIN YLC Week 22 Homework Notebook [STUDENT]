{"cells":[{"cell_type":"markdown","source":["# **Homework 22: Natural Language Processing Review**\n","---\n","\n","### **Description**\n","In this week's homework, you will review how to use more advanced forms of neural nets to perform tasks in NLP such as classification.\n","\n","<br>\n","\n","### **Structure**\n","**Part 1**: IMDB Sentiment Classification\n","\n","\n","\n","\n","<br>\n","\n","### **Cheat Sheets**\n","[Natural Language Processing II](https://docs.google.com/document/d/1p3xVUL1F6SEkusCI4klPLYqQwCkVN5s00ZvJjBpiSqM/edit?usp=sharing)\n","\n","<br>\n","\n","**Before starting, run the code below to import all necessary functions and libraries.**"],"metadata":{"id":"dTIoQdGufbcb"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","\n","import tensorflow as tf\n","import numpy as np\n","import os\n","\n","from keras.models import Sequential\n","from keras.layers import *\n","from keras.optimizers import Adam, SGD\n","from keras.utils import to_categorical\n","from keras.preprocessing.image import ImageDataGenerator\n","\n","from sklearn.model_selection import train_test_split\n","\n","from random import choices\n","\n","import warnings\n","warnings.filterwarnings('ignore')"],"metadata":{"id":"yWd0JEE9fmrW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<a name=\"p1\"></a>\n","\n","---\n","## **Part 1: IMDB Sentiment Classification**\n","---\n","\n","In this part we will focus on building a CNN model using the IMDB sentiment classification dataset. This is a dataset of 25,000 movie reviews with sentiment labels: 0 for negative and 1 for positive.\n","\n","<br>\n","\n","\n","**Run the code provided below to import the dataset.**"],"metadata":{"id":"Tnnv9dfa0I-T"}},{"cell_type":"code","source":["url = 'https://docs.google.com/spreadsheets/d/e/2PACX-1vTdgncgNHtppfS89LHOh1kGl5tYzoEUrUwmOPOQF7mQ0U5Rzba27H45imvZ06_J2x0-wCJySylP5V3_/pub?gid=1712575053&single=true&output=csv'\n","\n","df = pd.read_csv(url)\n","df.head()\n","\n","x_train, x_test, y_train, y_test = train_test_split(df[\"review\"], df[\"sentiment\"], test_size = 0.2, random_state = 42)\n","\n","x_train = np.array(x_train)\n","x_test = np.array(x_test)\n","y_train = to_categorical(y_train)\n","y_test = to_categorical(y_test)"],"metadata":{"id":"ACX4sfJW0bq5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Problem #1.1: Create the `TextVectorization` layer**\n","\n","\n","To get started, let's create a `TextVectorization` layer to vectorize this data.\n","\n","Specifically,\n","1. Initialize the layer with the specified parameters.\n","\n","2. Adapt the layer to the training data.\n","\n","3. Look at the newly built vocabulary."],"metadata":{"id":"HuGSwhDc0e1c"}},{"cell_type":"markdown","source":["#### **1. Initialize the layer with the specified parameters.**\n","\n","* The vocabulary should be at most 5000 words.\n","* The layer's output should always be 64 integers."],"metadata":{"id":"JlbOhj9L0hdD"}},{"cell_type":"code","source":[],"metadata":{"id":"r4CA7F9W0l4S"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **2. Adapt the layer to the training data.**"],"metadata":{"id":"3SgPJHmy0plM"}},{"cell_type":"code","source":[],"metadata":{"id":"sGMMXLG50ruI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **3. Look at the newly built vocabulary.**"],"metadata":{"id":"7KWwtA--0ue-"}},{"cell_type":"code","source":[],"metadata":{"id":"t_7qWqMl0xAp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Problem #1.2: Build and Train a Dense model**\n","\n","Complete the code below to build a model with the following layers.\n","\n","An Embedding layer such that:\n","- The vocabulary contains 5000 tokens.\n","- The input length corresponds to the output of the vectorization layer.\n","- The number of outputs per input is 128.\n","\n","<br>\n","\n","Hidden layers such that:\n","\n","- There's at least one Dense layer.\n","\n","<br>\n","\n","A Dense layer for outputting classification probabilities for \"negative\" or \"positive\" labels."],"metadata":{"id":"XH9dVdBW1D4w"}},{"cell_type":"code","source":[],"metadata":{"id":"tr-vmrCI1_mU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This other alternative includes building the model with CNN.\n","**Which architecture performs better?**"],"metadata":{"id":"_UHdf2m15v1u"}},{"cell_type":"code","source":["# [OPTIONAL] USING CNNs\n","model = Sequential()\n","\n","# Input, Vectorization, and Embedding Layers\n","model.add(Input(shape=(1,), dtype=tf.string))\n","model.add(vectorize_layer)\n","model.add(Embedding(input_dim = 5000, output_dim = 128, input_length = 64))\n","\n","# Hidden Layers\n","model.add(Conv1D(filters = 16, kernel_size = 4, activation = 'relu'))\n","model.add(MaxPooling1D(pool_size = 3))\n","model.add(Flatten())\n","\n","# Output Layer\n","model.add(Dense(2, activation = 'softmax'))\n","\n","\n","\n","# Printing Structure\n","for layer in model.layers:\n","  print(str(layer.input_shape) + \" -> \" + str(layer.output_shape))\n","print(\"\\n\\n\\n\")\n","\n","\n","\n","# Fitting\n","opt = Adam(learning_rate = 0.001)\n","model.compile(optimizer = opt, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n","model.fit(x_train, y_train, epochs = 5, batch_size = 256)\n","\n","\n","# Evaluating\n","print(\"\\n\\n\\n\")\n","model.evaluate(x_train, y_train)\n","model.evaluate(x_test, y_test)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LSM9qX412zyL","outputId":"49a68578-1284-42a9-d138-aa8616005f33","executionInfo":{"status":"ok","timestamp":1705952385460,"user_tz":480,"elapsed":47192,"user":{"displayName":"Angelique Membrido","userId":"04984385250682432912"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(None, 1) -> (None, 64)\n","(None, 64) -> (None, 64, 128)\n","(None, 64, 128) -> (None, 61, 16)\n","(None, 61, 16) -> (None, 20, 16)\n","(None, 20, 16) -> (None, 320)\n","(None, 320) -> (None, 2)\n","\n","\n","\n","\n","Epoch 1/5\n","157/157 [==============================] - 15s 79ms/step - loss: 0.5613 - accuracy: 0.7012\n","Epoch 2/5\n","157/157 [==============================] - 7s 45ms/step - loss: 0.3842 - accuracy: 0.8267\n","Epoch 3/5\n","157/157 [==============================] - 4s 25ms/step - loss: 0.3238 - accuracy: 0.8609\n","Epoch 4/5\n","157/157 [==============================] - 3s 22ms/step - loss: 0.2534 - accuracy: 0.9010\n","Epoch 5/5\n","157/157 [==============================] - 4s 25ms/step - loss: 0.1694 - accuracy: 0.9478\n","\n","\n","\n","\n","1250/1250 [==============================] - 6s 4ms/step - loss: 0.0943 - accuracy: 0.9885\n","313/313 [==============================] - 2s 5ms/step - loss: 0.5066 - accuracy: 0.7849\n"]},{"output_type":"execute_result","data":{"text/plain":["[0.5066009759902954, 0.7849000096321106]"]},"metadata":{},"execution_count":7}]},{"cell_type":"markdown","metadata":{"id":"7dzC09dLlEhm"},"source":["---\n","#End of notebook\n","\n","Â© 2024 The Coding School, All rights reserved"]}],"metadata":{"colab":{"provenance":[{"file_id":"1VbvC84SrV5ZOKxogfQXvwey0-q3TipJY","timestamp":1678175654957}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"nbformat":4,"nbformat_minor":0}