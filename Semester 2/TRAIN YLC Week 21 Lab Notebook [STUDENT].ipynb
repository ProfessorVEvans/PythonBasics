{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["zuSSFz900B2J","W4UR3nMoySPo","GS782-N7iKE4","NvqXQFlj4bl5","nfEdp9Et4xBh","d3zfKfgAiKE4","Bif_POk3iKE5","ovHm-Icy2iZv","R-rxKJXJFu-7"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["# **Lab 21: Natural Language Processing II**\n","---\n","\n","### **Description**\n","In today's lab, we have another text classification task, but this time we will be using **embeddings.** For this project, we will be working with a dataset of BBC News articles classified by topic.\n","\n","<br>\n","\n","### **Lab Structure**\n","\n","**Part 1**: [Text Classification of BBC Articles](#p1)\n","\n","**Part 2**: [Convolutional Neural Networks](#p2)\n","\n","\n","**Part 3**: [[OPTIONAL] IMDB Sentiment Classification](#p3)\n","\n","<br>\n","\n","### **Goals**\n","By the end of this lab, you will:\n","* Understand how to apply embedding layers in models.\n","* Compare a fully connected network to a CNN for text classification with embeddings.\n","\n","<br>\n","\n","### **Cheat Sheets**\n","[Natural Language Processing II](https://docs.google.com/document/d/1p3xVUL1F6SEkusCI4klPLYqQwCkVN5s00ZvJjBpiSqM/edit?usp=sharing)\n","\n","<br>\n","\n","**Before starting, run the code below to import all necessary functions and libraries.**"],"metadata":{"id":"3u6a3u2FYvkQ"}},{"cell_type":"code","source":["!pip install lime\n","\n","from lime import lime_text\n","import numpy as np\n","import pandas as pd\n","\n","import tensorflow as tf\n","import numpy as np\n","import os\n","\n","from keras.models import Sequential\n","from keras.layers import *\n","from keras.optimizers import Adam, SGD\n","from keras.utils import to_categorical\n","from keras.preprocessing.image import ImageDataGenerator\n","\n","from sklearn.model_selection import train_test_split\n","\n","from random import choices\n","\n","import warnings\n","warnings.filterwarnings('ignore')"],"metadata":{"id":"yWd0JEE9fmrW","colab":{"base_uri":"https://localhost:8080/"},"outputId":"cf81c54e-2749-4a83-8b57-b5be271c2e44","executionInfo":{"status":"ok","timestamp":1704488100226,"user_tz":480,"elapsed":7617,"user":{"displayName":"Angelique Membrido","userId":"04984385250682432912"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting lime\n","  Downloading lime-0.2.0.1.tar.gz (275 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m275.7/275.7 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from lime) (3.7.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from lime) (1.23.5)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from lime) (1.11.4)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from lime) (4.66.1)\n","Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.10/dist-packages (from lime) (1.2.2)\n","Requirement already satisfied: scikit-image>=0.12 in /usr/local/lib/python3.10/dist-packages (from lime) (0.19.3)\n","Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.12->lime) (3.2.1)\n","Requirement already satisfied: pillow!=7.1.0,!=7.1.1,!=8.3.0,>=6.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.12->lime) (9.4.0)\n","Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.12->lime) (2.31.6)\n","Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.12->lime) (2023.12.9)\n","Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.12->lime) (1.5.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.12->lime) (23.2)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.18->lime) (1.3.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.18->lime) (3.2.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lime) (1.2.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lime) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lime) (4.47.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lime) (1.4.5)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lime) (3.1.1)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lime) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->lime) (1.16.0)\n","Building wheels for collected packages: lime\n","  Building wheel for lime (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for lime: filename=lime-0.2.0.1-py3-none-any.whl size=283835 sha256=33be6d59f1e398381ad1f94cd08b7129dc48daa49aaf98cff048ba9568a4082b\n","  Stored in directory: /root/.cache/pip/wheels/fd/a2/af/9ac0a1a85a27f314a06b39e1f492bee1547d52549a4606ed89\n","Successfully built lime\n","Installing collected packages: lime\n","Successfully installed lime-0.2.0.1\n"]}]},{"cell_type":"markdown","source":["<a name=\"p1\"></a>\n","\n","---\n","## **Part 1: Text Classification of BBC Articles**\n","---\n","\n","In this section, we'll apply our text classification knowledge to a corpus of BBC articles. Your task is to develop a model that categorizes articles based on snippets of text, assigning each to a specific category. Unlike previous labs where we focused on visual data, here we'll use neural networks with traditional Dense layers to process and classify text data.\n","\n","<br>\n","\n","\n","\n","**Run the code provided below to import the dataset.**"],"metadata":{"id":"zuSSFz900B2J"}},{"cell_type":"code","source":["dataset = pd.read_csv('https://docs.google.com/spreadsheets/d/e/2PACX-1vRRiQ1DUkUxk31YpaHA2i9QtwGq_VGXiy86z7l3aT9v5zoB6M7a-2M2qlYckr1C_ZG6StBELlU_hD3S/pub?output=csv')"],"metadata":{"id":"U2ek2hb10flg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Problem #1.1: Determine the number of categories**\n","\n","\n","Using any necessary pandas functions or attributes, determine the total number of unique categories that the texts are assigned to.\n","\n","**In the cell code below, display the first few rows in the dataset.**\n"],"metadata":{"id":"W4UR3nMoySPo"}},{"cell_type":"code","source":[],"metadata":{"id":"9bL0s5LH4mB5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's now see which of the categories in our dataset are unique."],"metadata":{"id":"VIO4pIXXI846"}},{"cell_type":"code","source":["print(dataset[\"category\"].unique())\n","print(len(dataset[\"category\"].unique()))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q1KvWpjI4noW","outputId":"3c69fbf4-f1d9-49fc-ac16-4e6c3926c089"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['business' 'entertainment' 'politics' 'sport' 'tech']\n","5\n"]}]},{"cell_type":"markdown","source":["**Question:** How many unique categories do we have from the above output? What are the unique categories?"],"metadata":{"id":"JcZ388FPJFk0"}},{"cell_type":"markdown","source":["### **Problem #1.2: Split the data into training and test sets**\n","\n","\n","Determine the correct variables to use as the feature(s) and label here, making sure to provide numerical labels for the neural network to predict."],"metadata":{"id":"NvqXQFlj4bl5"}},{"cell_type":"code","source":[],"metadata":{"id":"nkrcRMCc4yvj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Split the dataset into features and labels\n","x = dataset['text'].values\n","y = dataset['category_id'].values\n","\n","# Split the dataset into training and test sets\n","x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)\n","\n","y_train = to_categorical(y_train)\n","y_test = to_categorical(y_test)"],"metadata":{"id":"Pe4Cg-y9y7WV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Problem #1.3: Create the `TextVectorization` layer**\n","\n","\n","To get started building the neural network, create a `TextVectorization` layer to vectorize this data.\n","\n","Specifically,\n","1. Initialize the layer with the specified parameters.\n","\n","2. Adapt the layer to the training data.\n","\n","3. Look at the newly built vocabulary."],"metadata":{"id":"LTm3147qiKE4"}},{"cell_type":"markdown","source":["#### **1.Initialize the layer with the specified parameters.**\n","\n","* The vocabulary should be at most 10000 words.\n","* The layer's output should always be 20 integers."],"metadata":{"id":"d3zfKfgAiKE4"}},{"cell_type":"code","source":[],"metadata":{"id":"0TUaQw3L7W1f"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **2. Adapt the layer to the training data.**"],"metadata":{"id":"Bif_POk3iKE5"}},{"cell_type":"code","source":[],"metadata":{"id":"bsnDnHGFiKE5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **3. Look at the newly built vocabulary.**"],"metadata":{"id":"ovHm-Icy2iZv"}},{"cell_type":"code","source":[],"metadata":{"id":"bnAtuO3B2iZw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Problem #1.4: Build the model**\n","\n","\n","Complete the code below to build a model with the following layers.\n","\n","An `Embedding` layer such that:\n","* The vocabulary contains 10000 tokens.\n","* The input length corresponds to the output of the vectorization layer.\n","* The number of outputs per input is 200.\n","\n","<br>\n","\n","Two `Dense` layers with a number of neurons and activation function that you choose. We recommend you try a few options.\n","\n","<br>\n","\n","A `Dense` layer for outputting classification probabilities for each of the possible categories.\n","\n","\n","*Hint: If you're not sure which activation function to use, use `relu`.*"],"metadata":{"id":"_ULrKEoKiKE6"}},{"cell_type":"code","source":["model = Sequential()\n","\n","# Input, Vectorization, and Embedding Layers\n","model.add(Input(shape=(1,), dtype=tf.string))\n","model.add(vectorize_layer)\n","model.add(Embedding(# COMPLETE THIS LINE\n","\n","# Hidden Layers\n","model.add(# COMPLETE THIS LINE\n","\n","# Output Layer\n","model.add(# COMPLETE THIS LINE"],"metadata":{"id":"qliSJAYCiKE6","colab":{"base_uri":"https://localhost:8080/","height":135},"outputId":"88ea0fac-1b81-4dcd-89b4-ad46aad58d77"},"execution_count":null,"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"ignored","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-10-6c51cf3a7eaf>\"\u001b[0;36m, line \u001b[0;32m12\u001b[0m\n\u001b[0;31m    model.add(# COMPLETE THIS LINE\u001b[0m\n\u001b[0m                                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"]}]},{"cell_type":"markdown","source":["### **Problem #1.5: Compile and fit the model**\n","\n","Using standard parameters for classification, compile and train 8yncthis neural network using:\n","* A learning rate of 0.01.\n","* A batch size of 200.\n","* 5 epochs."],"metadata":{"id":"RMhg0mcliKE6"}},{"cell_type":"code","source":["opt = Adam(learning_rate = # COMPLETE THIS LINE\n","model.compile(optimizer = opt, loss = # COMPLETE THIS LINE\n","\n","model.fit(# COMPLETE THIS LINE"],"metadata":{"id":"GIWQr3UFiKE6","colab":{"base_uri":"https://localhost:8080/","height":135},"outputId":"2711843e-e3c8-4edc-ed85-f6698fd21bf7"},"execution_count":null,"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"ignored","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-12-e3a99cb8abb9>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    model.fit(# COMPLETE THIS LINE\u001b[0m\n\u001b[0m                                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"]}]},{"cell_type":"markdown","source":["### **Problem #1.6: Evaluate the model**\n","\n","\n","Now, evaluate the model for both the training and test sets."],"metadata":{"id":"rZDdIc2LiKE7"}},{"cell_type":"code","source":[],"metadata":{"id":"rtdeiWm7iKE7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<a name=\"p2\"></a>\n","\n","---\n","## **Part 2:  Convolutional Neural Networks**\n","---\n","\n","In this section, you will classify the articles using neural networks with `Conv1D` and `MaxPooling1D` hidden layers. Feel free to include `Dense` hidden layers too."],"metadata":{"id":"R-rxKJXJFu-7"}},{"cell_type":"markdown","source":["### **Problem #2.1: Create the highest performing model possible using CNNs**\n","\n","\n","Complete the code below to train a new model that is identical to the one above, except using any or all of the CNN layers that keras provides. The goal is to create a model that performs as well as possible on the *test set*."],"metadata":{"id":"Ek9OyYc_8z6b"}},{"cell_type":"code","source":["model = Sequential()\n","\n","# Input, Vectorization, and Embedding Layers\n","# COMPLETE THIS CODE\n","\n","\n","# Hidden Layers\n","# COMPLETE THIS CODE\n","\n","\n","# Output Layer\n","# COMPLETE THIS CODE\n","\n","\n","# Fitting\n","# COMPLETE THIS CODE\n","\n","\n","# Evaluating\n","print(\"\\n\\n\\n\")\n","# COMPLETE THIS CODE"],"metadata":{"id":"IqhKG-xN8z6h"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<a name=\"p3\"></a>\n","\n","---\n","## **Part 3: [OPTIONAL] IMDB Sentiment Classification**\n","---\n","\n","In this part we will focus on building a CNN model using the IMDB sentiment classification dataset. This is a dataset of 25,000 movie reviews with sentiment labels: 0 for negative and 1 for positive.\n","\n","<br>\n","\n","\n","**Run the code provided below to import the dataset.**"],"metadata":{"id":"Tnnv9dfa0I-T"}},{"cell_type":"code","source":["url = 'https://docs.google.com/spreadsheets/d/e/2PACX-1vTdgncgNHtppfS89LHOh1kGl5tYzoEUrUwmOPOQF7mQ0U5Rzba27H45imvZ06_J2x0-wCJySylP5V3_/pub?gid=1712575053&single=true&output=csv'\n","\n","df = pd.read_csv(url)\n","df.head()\n","\n","x_train, x_test, y_train, y_test = train_test_split(df[\"review\"], df[\"sentiment\"], test_size = 0.2, random_state = 42)\n","\n","x_train = np.array(x_train)\n","x_test = np.array(x_test)\n","y_train = to_categorical(y_train)\n","y_test = to_categorical(y_test)"],"metadata":{"id":"ACX4sfJW0bq5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Problem #3.1: Create the `TextVectorization` layer**\n","\n","\n","To get started, let's create a `TextVectorization` layer to vectorize this data.\n","\n","Specifically,\n","1. Initialize the layer with the specified parameters.\n","\n","2. Adapt the layer to the training data.\n","\n","3. Look at the newly built vocabulary."],"metadata":{"id":"HuGSwhDc0e1c"}},{"cell_type":"markdown","source":["#### **1. Initialize the layer with the specified parameters.**\n","\n","* The vocabulary should be at most 5000 words.\n","* The layer's output should always be 64 integers."],"metadata":{"id":"JlbOhj9L0hdD"}},{"cell_type":"code","source":[],"metadata":{"id":"r4CA7F9W0l4S"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **2. Adapt the layer to the training data.**"],"metadata":{"id":"3SgPJHmy0plM"}},{"cell_type":"code","source":[],"metadata":{"id":"sGMMXLG50ruI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **3. Look at the newly built vocabulary.**"],"metadata":{"id":"7KWwtA--0ue-"}},{"cell_type":"code","source":[],"metadata":{"id":"t_7qWqMl0xAp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Problem #3.2: Build and Train a Dense model**\n","\n","Complete the code below to build a model with the following layers.\n","\n","An Embedding layer such that:\n","- The vocabulary contains 5000 tokens.\n","- The input length corresponds to the output of the vectorization layer.\n","- The number of outputs per input is 128.\n","\n","<br>\n","\n","Hidden layers such that:\n","\n","- There's at least one Dense layer.\n","\n","<br>\n","\n","A Dense layer for outputting classification probabilities for \"negative\" or \"positive\" labels."],"metadata":{"id":"XH9dVdBW1D4w"}},{"cell_type":"code","source":[],"metadata":{"id":"tr-vmrCI1_mU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This other alternative includes building the model with CNN.\n","**Which architecture performs better?**"],"metadata":{"id":"_UHdf2m15v1u"}},{"cell_type":"code","source":["# [OPTIONAL] USING CNNs\n","model = Sequential()\n","\n","# Input, Vectorization, and Embedding Layers\n","model.add(Input(shape=(1,), dtype=tf.string))\n","model.add(vectorize_layer)\n","model.add(Embedding(input_dim = 5000, output_dim = 128, input_length = 64))\n","\n","# Hidden Layers\n","model.add(Conv1D(filters = 16, kernel_size = 4, activation = 'relu'))\n","model.add(MaxPooling1D(pool_size = 3))\n","model.add(Flatten())\n","\n","# Output Layer\n","model.add(Dense(2, activation = 'softmax'))\n","\n","\n","\n","# Printing Structure\n","for layer in model.layers:\n","  print(str(layer.input_shape) + \" -> \" + str(layer.output_shape))\n","print(\"\\n\\n\\n\")\n","\n","\n","\n","# Fitting\n","opt = Adam(learning_rate = 0.001)\n","model.compile(optimizer = opt, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n","model.fit(x_train, y_train, epochs = 5, batch_size = 256)\n","\n","\n","# Evaluating\n","print(\"\\n\\n\\n\")\n","model.evaluate(x_train, y_train)\n","model.evaluate(x_test, y_test)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LSM9qX412zyL","outputId":"de7fb0cf-a6b0-4ad8-d0af-3671ca00cfcb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(None, 1) -> (None, 64)\n","(None, 64) -> (None, 64, 128)\n","(None, 64, 128) -> (None, 61, 16)\n","(None, 61, 16) -> (None, 20, 16)\n","(None, 20, 16) -> (None, 320)\n","(None, 320) -> (None, 2)\n","\n","\n","\n","\n","Epoch 1/5\n","157/157 [==============================] - 6s 26ms/step - loss: 0.5786 - accuracy: 0.6819\n","Epoch 2/5\n","157/157 [==============================] - 3s 17ms/step - loss: 0.3879 - accuracy: 0.8229\n","Epoch 3/5\n","157/157 [==============================] - 3s 21ms/step - loss: 0.3235 - accuracy: 0.8619\n","Epoch 4/5\n","157/157 [==============================] - 3s 16ms/step - loss: 0.2545 - accuracy: 0.9027\n","Epoch 5/5\n","157/157 [==============================] - 3s 21ms/step - loss: 0.1674 - accuracy: 0.9501\n","\n","\n","\n","\n","1250/1250 [==============================] - 8s 6ms/step - loss: 0.0919 - accuracy: 0.9890\n","313/313 [==============================] - 2s 6ms/step - loss: 0.5065 - accuracy: 0.7865\n"]},{"output_type":"execute_result","data":{"text/plain":["[0.5064528584480286, 0.7864999771118164]"]},"metadata":{},"execution_count":51}]},{"cell_type":"markdown","source":["---\n","###© 2024 The Coding School, All rights reserved"],"metadata":{"id":"8zFgD9Jx1ooV"}}]}