{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["zuSSFz900B2J","W4UR3nMoySPo","GS782-N7iKE4","NvqXQFlj4bl5","nfEdp9Et4xBh","d3zfKfgAiKE4","Bif_POk3iKE5","ovHm-Icy2iZv","R-rxKJXJFu-7"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["# **Lab 21: Natural Language Processing II**\n","---\n","\n","### **Description**\n","In today's lab, we have another text classification task, but this time we will be using **embeddings.** For this project, we will be working with a dataset of BBC News articles classified by topic.\n","\n","<br>\n","\n","### **Lab Structure**\n","\n","**Part 1**: [Text Classification of BBC Articles](#p1)\n","\n","**Part 2**: [Convolutional Neural Networks](#p2)\n","\n","\n","**Part 3**: [[OPTIONAL] IMDB Sentiment Classification](#p3)\n","\n","<br>\n","\n","### **Goals**\n","By the end of this lab, you will:\n","* Understand how to apply embedding layers in models.\n","* Compare a fully connected network to a CNN for text classification with embeddings.\n","\n","<br>\n","\n","### **Cheat Sheets**\n","[Natural Language Processing II](https://docs.google.com/document/d/1p3xVUL1F6SEkusCI4klPLYqQwCkVN5s00ZvJjBpiSqM/edit?usp=sharing)\n","\n","<br>\n","\n","**Before starting, run the code below to import all necessary functions and libraries.**"],"metadata":{"id":"3u6a3u2FYvkQ"}},{"cell_type":"code","source":["!pip install lime\n","\n","from lime import lime_text\n","import numpy as np\n","import pandas as pd\n","\n","import tensorflow as tf\n","import numpy as np\n","import os\n","\n","from keras.models import Sequential\n","from keras.layers import *\n","from keras.optimizers import Adam, SGD\n","from keras.utils import to_categorical\n","from keras.preprocessing.image import ImageDataGenerator\n","\n","from sklearn.model_selection import train_test_split\n","\n","from random import choices\n","\n","import warnings\n","warnings.filterwarnings('ignore')"],"metadata":{"id":"yWd0JEE9fmrW","colab":{"base_uri":"https://localhost:8080/"},"outputId":"cf81c54e-2749-4a83-8b57-b5be271c2e44","executionInfo":{"status":"ok","timestamp":1704488100226,"user_tz":480,"elapsed":7617,"user":{"displayName":"Angelique Membrido","userId":"04984385250682432912"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting lime\n","  Downloading lime-0.2.0.1.tar.gz (275 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m275.7/275.7 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from lime) (3.7.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from lime) (1.23.5)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from lime) (1.11.4)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from lime) (4.66.1)\n","Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.10/dist-packages (from lime) (1.2.2)\n","Requirement already satisfied: scikit-image>=0.12 in /usr/local/lib/python3.10/dist-packages (from lime) (0.19.3)\n","Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.12->lime) (3.2.1)\n","Requirement already satisfied: pillow!=7.1.0,!=7.1.1,!=8.3.0,>=6.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.12->lime) (9.4.0)\n","Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.12->lime) (2.31.6)\n","Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.12->lime) (2023.12.9)\n","Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.12->lime) (1.5.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.12->lime) (23.2)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.18->lime) (1.3.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.18->lime) (3.2.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lime) (1.2.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lime) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lime) (4.47.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lime) (1.4.5)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lime) (3.1.1)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lime) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->lime) (1.16.0)\n","Building wheels for collected packages: lime\n","  Building wheel for lime (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for lime: filename=lime-0.2.0.1-py3-none-any.whl size=283835 sha256=33be6d59f1e398381ad1f94cd08b7129dc48daa49aaf98cff048ba9568a4082b\n","  Stored in directory: /root/.cache/pip/wheels/fd/a2/af/9ac0a1a85a27f314a06b39e1f492bee1547d52549a4606ed89\n","Successfully built lime\n","Installing collected packages: lime\n","Successfully installed lime-0.2.0.1\n"]}]},{"cell_type":"markdown","source":["<a name=\"p1\"></a>\n","\n","---\n","## **Part 1: Text Classification of BBC Articles**\n","---\n","\n","In this section, we'll apply our text classification knowledge to a corpus of BBC articles. Your task is to develop a model that categorizes articles based on snippets of text, assigning each to a specific category. Unlike previous labs where we focused on visual data, here we'll use neural networks with traditional Dense layers to process and classify text data.\n","\n","<br>\n","\n","\n","\n","**Run the code provided below to import the dataset.**"],"metadata":{"id":"zuSSFz900B2J"}},{"cell_type":"code","source":["dataset = pd.read_csv('https://docs.google.com/spreadsheets/d/e/2PACX-1vRRiQ1DUkUxk31YpaHA2i9QtwGq_VGXiy86z7l3aT9v5zoB6M7a-2M2qlYckr1C_ZG6StBELlU_hD3S/pub?output=csv')"],"metadata":{"id":"U2ek2hb10flg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Problem #1.1: Determine the number of categories**\n","\n","\n","Using any necessary pandas functions or attributes, determine the total number of unique categories that the texts are assigned to.\n","\n","**In the cell code below, display the first few rows in the dataset.**\n"],"metadata":{"id":"W4UR3nMoySPo"}},{"cell_type":"code","source":[],"metadata":{"id":"9bL0s5LH4mB5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Solution**\n"],"metadata":{"id":"GS782-N7iKE4"}},{"cell_type":"code","source":["dataset.head()"],"metadata":{"id":"uXzBNSBHwA8B","colab":{"base_uri":"https://localhost:8080/","height":449},"outputId":"51b5d638-0413-4e24-8411-626526c1adfc","executionInfo":{"status":"ok","timestamp":1704488105040,"user_tz":480,"elapsed":178,"user":{"displayName":"Angelique Membrido","userId":"04984385250682432912"}}},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      text  \\\n","0  libya takes 1bn unfrozen funds libya withdrawn 1bn assets assets previously frozen libyan central bank came lifted trade ban reward tripoli giving weapons mass destruction vowing compensate lockerbie victims original size libya funds 400 central bank reuters withdrawal mean libya ties process opening accounts banks united states central bank vice president farhat omar ben gadaravice previously frozen assets invested countries believed included equity holdings banks ban trade economic activity tripoli imposed president ronald regan 1986 series deemed terrorist acts 1988 lockerbie air crash ...   \n","1                                saudi investor picks savoy famous savoy hotel sold group combining saudi billionaire investor prince alwaleed bin talal unit hbos bank financial details includes nearby simpson strand restaurant disclosed seller irish based property quinlan private bought savoy berkeley claridge connaught £ 750 m prince alwaleed hotel investments luxury george v paris substantial stakes fairmont hotels resorts manage savoy simpson strand seasons fairmont planned invest 48 m £ 26 m renovating parts savoy including river room suites views river thames completed summer 2006 fairmont   \n","2  tate lyle boss bags award tate lyle chief executive named businessman leading magazine iain ferguson awarded title publication forbes returning venerable manufacturers 100 sugar group absent ftse 100 seven years mr ferguson helped return growth tate shares leapt 55 boosted firming sugar prices sales artificial sweeteners years sagging stock price seven hiatus ftse 100 venerable manufacturers returned vaunted index forbes mr ferguson took helm 2003 spending career consumer goods giant unilever tate lyle original member historic ft 30 index 1935 operates 41 factories 20 additional production...   \n","3  uk economy facing major risks uk manufacturing sector continue face challenges years british chamber commerce bcc quarterly survey found exports picked months 2004 levels years rise came despite exchange rates cited major concern bcc found uk economy faced major risks warned growth set slow recently forecast economic growth slow 2004 little 5 2006 manufacturers domestic sales growth fell slightly quarter survey 5 196 firms found employment manufacturing fell job expectations lowest level despite positive export sector worrying signs manufacturing bcc results reinforce concern sector persis...   \n","4  aids climate davos agenda climate change fight aids leading list concerns day world economic forum swiss resort davos 000 business political leaders globe listen uk prime tony blair opening speech wednesday mr blair focus africa development global warming earlier day came update efforts million people anti aids drugs 2005 world health organisation 700 000 people poor countries extending drugs 440 000 earlier amounting million needed 2bn funding gap stood hitting 2005 target themes stressed mr blair attendance announced minute wants dominate uk chairmanship g8 industrialised states issues d...   \n","\n","   category  category_id  \n","0  business            0  \n","1  business            0  \n","2  business            0  \n","3  business            0  \n","4  business            0  "],"text/html":["\n","  <div id=\"df-082ed80a-f546-4cf2-828b-77ee75c56a59\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>category</th>\n","      <th>category_id</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>libya takes 1bn unfrozen funds libya withdrawn 1bn assets assets previously frozen libyan central bank came lifted trade ban reward tripoli giving weapons mass destruction vowing compensate lockerbie victims original size libya funds 400 central bank reuters withdrawal mean libya ties process opening accounts banks united states central bank vice president farhat omar ben gadaravice previously frozen assets invested countries believed included equity holdings banks ban trade economic activity tripoli imposed president ronald regan 1986 series deemed terrorist acts 1988 lockerbie air crash ...</td>\n","      <td>business</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>saudi investor picks savoy famous savoy hotel sold group combining saudi billionaire investor prince alwaleed bin talal unit hbos bank financial details includes nearby simpson strand restaurant disclosed seller irish based property quinlan private bought savoy berkeley claridge connaught £ 750 m prince alwaleed hotel investments luxury george v paris substantial stakes fairmont hotels resorts manage savoy simpson strand seasons fairmont planned invest 48 m £ 26 m renovating parts savoy including river room suites views river thames completed summer 2006 fairmont</td>\n","      <td>business</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>tate lyle boss bags award tate lyle chief executive named businessman leading magazine iain ferguson awarded title publication forbes returning venerable manufacturers 100 sugar group absent ftse 100 seven years mr ferguson helped return growth tate shares leapt 55 boosted firming sugar prices sales artificial sweeteners years sagging stock price seven hiatus ftse 100 venerable manufacturers returned vaunted index forbes mr ferguson took helm 2003 spending career consumer goods giant unilever tate lyle original member historic ft 30 index 1935 operates 41 factories 20 additional production...</td>\n","      <td>business</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>uk economy facing major risks uk manufacturing sector continue face challenges years british chamber commerce bcc quarterly survey found exports picked months 2004 levels years rise came despite exchange rates cited major concern bcc found uk economy faced major risks warned growth set slow recently forecast economic growth slow 2004 little 5 2006 manufacturers domestic sales growth fell slightly quarter survey 5 196 firms found employment manufacturing fell job expectations lowest level despite positive export sector worrying signs manufacturing bcc results reinforce concern sector persis...</td>\n","      <td>business</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>aids climate davos agenda climate change fight aids leading list concerns day world economic forum swiss resort davos 000 business political leaders globe listen uk prime tony blair opening speech wednesday mr blair focus africa development global warming earlier day came update efforts million people anti aids drugs 2005 world health organisation 700 000 people poor countries extending drugs 440 000 earlier amounting million needed 2bn funding gap stood hitting 2005 target themes stressed mr blair attendance announced minute wants dominate uk chairmanship g8 industrialised states issues d...</td>\n","      <td>business</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-082ed80a-f546-4cf2-828b-77ee75c56a59')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-082ed80a-f546-4cf2-828b-77ee75c56a59 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-082ed80a-f546-4cf2-828b-77ee75c56a59');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-df455713-5ffa-4d8a-af59-347e074342f1\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-df455713-5ffa-4d8a-af59-347e074342f1')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-df455713-5ffa-4d8a-af59-347e074342f1 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":10}]},{"cell_type":"markdown","source":["Let's now see which of the categories in our dataset are unique."],"metadata":{"id":"VIO4pIXXI846"}},{"cell_type":"code","source":["print(dataset[\"category\"].unique())\n","print(len(dataset[\"category\"].unique()))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q1KvWpjI4noW","outputId":"3c69fbf4-f1d9-49fc-ac16-4e6c3926c089"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['business' 'entertainment' 'politics' 'sport' 'tech']\n","5\n"]}]},{"cell_type":"markdown","source":["**Question:** How many unique categories do we have from the above output? What are the unique categories?"],"metadata":{"id":"JcZ388FPJFk0"}},{"cell_type":"markdown","source":["### **Problem #1.2: Split the data into training and test sets**\n","\n","\n","Determine the correct variables to use as the feature(s) and label here, making sure to provide numerical labels for the neural network to predict."],"metadata":{"id":"NvqXQFlj4bl5"}},{"cell_type":"code","source":[],"metadata":{"id":"nkrcRMCc4yvj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Solution**\n"],"metadata":{"id":"nfEdp9Et4xBh"}},{"cell_type":"code","source":["dataset['category'].unique()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iThJzxNLABR2","outputId":"8aa47c4e-91ea-4225-c39d-67d11dc5142c","executionInfo":{"status":"ok","timestamp":1704488109372,"user_tz":480,"elapsed":122,"user":{"displayName":"Angelique Membrido","userId":"04984385250682432912"}}},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array(['business', 'entertainment', 'politics', 'sport', 'tech'],\n","      dtype=object)"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["# Split the dataset into features and labels\n","x = dataset['text'].values\n","y = dataset['category_id'].values\n","\n","# Split the dataset into training and test sets\n","x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)\n","\n","y_train = to_categorical(y_train)\n","y_test = to_categorical(y_test)"],"metadata":{"id":"Pe4Cg-y9y7WV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Problem #1.3: Create the `TextVectorization` layer**\n","\n","\n","To get started building the neural network, create a `TextVectorization` layer to vectorize this data.\n","\n","Specifically,\n","1. Initialize the layer with the specified parameters.\n","\n","2. Adapt the layer to the training data.\n","\n","3. Look at the newly built vocabulary."],"metadata":{"id":"LTm3147qiKE4"}},{"cell_type":"markdown","source":["#### **1.Initialize the layer with the specified parameters.**\n","\n","* The vocabulary should be at most 10000 words.\n","* The layer's output should always be 20 integers."],"metadata":{"id":"d3zfKfgAiKE4"}},{"cell_type":"code","source":[],"metadata":{"id":"0TUaQw3L7W1f"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###### **Solution**\n"],"metadata":{"id":"5QSyg_MS5nv6"}},{"cell_type":"code","source":["vectorize_layer = TextVectorization(max_tokens = 10000, output_mode = 'int', output_sequence_length = 20)"],"metadata":{"id":"b-kOeNYRiKE5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **2. Adapt the layer to the training data.**"],"metadata":{"id":"Bif_POk3iKE5"}},{"cell_type":"code","source":[],"metadata":{"id":"bsnDnHGFiKE5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### **Solution**\n"],"metadata":{"id":"jJRRdYCuiKE5"}},{"cell_type":"code","source":["vectorize_layer.adapt(x_train)"],"metadata":{"id":"jIOkTtuniKE5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **3. Look at the newly built vocabulary.**"],"metadata":{"id":"ovHm-Icy2iZv"}},{"cell_type":"code","source":[],"metadata":{"id":"bnAtuO3B2iZw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### **Solution**\n"],"metadata":{"id":"fQ4o2Bkc2iZx"}},{"cell_type":"code","source":["vectorize_layer.get_vocabulary()[0:30]"],"metadata":{"id":"lAvM6ntr2iZx","colab":{"base_uri":"https://localhost:8080/"},"outputId":"9b5434c2-4132-4dc7-b23f-c4f7de8c5d49","executionInfo":{"status":"ok","timestamp":1704488120433,"user_tz":480,"elapsed":129,"user":{"displayName":"Angelique Membrido","userId":"04984385250682432912"}}},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['',\n"," '[UNK]',\n"," 'mr',\n"," 'm',\n"," 'people',\n"," 'new',\n"," '£',\n"," 't',\n"," 'government',\n"," 'film',\n"," 'year',\n"," 'uk',\n"," 'music',\n"," 'game',\n"," 'world',\n"," 'best',\n"," 'labour',\n"," 'election',\n"," 'time',\n"," 'blair',\n"," '1',\n"," 'party',\n"," 'games',\n"," 'mobile',\n"," 'market',\n"," 'england',\n"," '000',\n"," 'tv',\n"," '3',\n"," '2']"]},"metadata":{},"execution_count":15}]},{"cell_type":"markdown","source":["### **Problem #1.4: Build the model**\n","\n","\n","Complete the code below to build a model with the following layers.\n","\n","An `Embedding` layer such that:\n","* The vocabulary contains 10000 tokens.\n","* The input length corresponds to the output of the vectorization layer.\n","* The number of outputs per input is 200.\n","\n","<br>\n","\n","Two `Dense` layers with a number of neurons and activation function that you choose. We recommend you try a few options.\n","\n","<br>\n","\n","A `Dense` layer for outputting classification probabilities for each of the possible categories.\n","\n","\n","*Hint: If you're not sure which activation function to use, use `relu`.*"],"metadata":{"id":"_ULrKEoKiKE6"}},{"cell_type":"code","source":["model = Sequential()\n","\n","# Input, Vectorization, and Embedding Layers\n","model.add(Input(shape=(1,), dtype=tf.string))\n","model.add(vectorize_layer)\n","model.add(Embedding(# COMPLETE THIS LINE\n","\n","# Hidden Layers\n","model.add(# COMPLETE THIS LINE\n","\n","# Output Layer\n","model.add(# COMPLETE THIS LINE"],"metadata":{"id":"qliSJAYCiKE6","colab":{"base_uri":"https://localhost:8080/","height":135},"outputId":"88ea0fac-1b81-4dcd-89b4-ad46aad58d77"},"execution_count":null,"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"ignored","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-10-6c51cf3a7eaf>\"\u001b[0;36m, line \u001b[0;32m12\u001b[0m\n\u001b[0;31m    model.add(# COMPLETE THIS LINE\u001b[0m\n\u001b[0m                                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"]}]},{"cell_type":"markdown","source":["#### **Solution**\n"],"metadata":{"id":"nu7H7YndiKE6"}},{"cell_type":"code","source":["model = Sequential()\n","\n","# Input, Vectorization, and Embedding Layers\n","model.add(Input(shape=(1,), dtype=tf.string))\n","model.add(vectorize_layer)\n","model.add(Embedding(input_dim = 10000, output_dim = 200, input_length = 50))\n","model.add(Flatten())\n","\n","# Hidden Layers\n","model.add(Dense(64, activation = 'relu'))\n","model.add(Dense(128, activation = 'relu'))\n","\n","# Output Layer\n","model.add(Dense(len(dataset['category'].unique()), activation = 'softmax'))"],"metadata":{"id":"7cIuAtj6iKE6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Problem #1.5: Compile and fit the model**\n","\n","Using standard parameters for classification, compile and train 8yncthis neural network using:\n","* A learning rate of 0.01.\n","* A batch size of 200.\n","* 5 epochs."],"metadata":{"id":"RMhg0mcliKE6"}},{"cell_type":"code","source":["opt = Adam(learning_rate = # COMPLETE THIS LINE\n","model.compile(optimizer = opt, loss = # COMPLETE THIS LINE\n","\n","model.fit(# COMPLETE THIS LINE"],"metadata":{"id":"GIWQr3UFiKE6","colab":{"base_uri":"https://localhost:8080/","height":135},"outputId":"2711843e-e3c8-4edc-ed85-f6698fd21bf7"},"execution_count":null,"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"ignored","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-12-e3a99cb8abb9>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    model.fit(# COMPLETE THIS LINE\u001b[0m\n\u001b[0m                                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"]}]},{"cell_type":"markdown","source":["#### **Solution**\n"],"metadata":{"id":"dlJj2eKsiKE6"}},{"cell_type":"code","source":["opt = Adam(learning_rate = 0.01)\n","model.compile(optimizer = opt, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n","\n","model.fit(x_train, y_train, epochs = 5, batch_size = 200)"],"metadata":{"id":"tzW1Uc4QiKE7","colab":{"base_uri":"https://localhost:8080/"},"outputId":"f557be2e-b56c-4a8e-aa91-ccd532efd24f","executionInfo":{"status":"ok","timestamp":1704488142239,"user_tz":480,"elapsed":10051,"user":{"displayName":"Angelique Membrido","userId":"04984385250682432912"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/5\n","9/9 [==============================] - 5s 151ms/step - loss: 1.1711 - accuracy: 0.5545\n","Epoch 2/5\n","9/9 [==============================] - 1s 150ms/step - loss: 0.0391 - accuracy: 0.9961\n","Epoch 3/5\n","9/9 [==============================] - 1s 148ms/step - loss: 0.0152 - accuracy: 0.9994\n","Epoch 4/5\n","9/9 [==============================] - 1s 91ms/step - loss: 5.3247e-04 - accuracy: 0.9994\n","Epoch 5/5\n","9/9 [==============================] - 1s 112ms/step - loss: 0.0124 - accuracy: 0.9983\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.src.callbacks.History at 0x7882a06f8b50>"]},"metadata":{},"execution_count":17}]},{"cell_type":"markdown","source":["### **Problem #1.6: Evaluate the model**\n","\n","\n","Now, evaluate the model for both the training and test sets."],"metadata":{"id":"rZDdIc2LiKE7"}},{"cell_type":"code","source":[],"metadata":{"id":"rtdeiWm7iKE7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Solution**\n"],"metadata":{"id":"qA1rlfoKiKE7"}},{"cell_type":"code","source":["model.evaluate(x_train, y_train)\n","model.evaluate(x_test, y_test)"],"metadata":{"id":"Tno6b9hUiKE7","colab":{"base_uri":"https://localhost:8080/"},"outputId":"412012f3-be21-4527-f01b-bc6fb9d88496","executionInfo":{"status":"ok","timestamp":1704488148896,"user_tz":480,"elapsed":602,"user":{"displayName":"Angelique Membrido","userId":"04984385250682432912"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["56/56 [==============================] - 0s 4ms/step - loss: 0.0032 - accuracy: 0.9994\n","14/14 [==============================] - 0s 4ms/step - loss: 1.9173 - accuracy: 0.9011\n"]},{"output_type":"execute_result","data":{"text/plain":["[1.917319416999817, 0.901123583316803]"]},"metadata":{},"execution_count":18}]},{"cell_type":"markdown","source":["<a name=\"p2\"></a>\n","\n","---\n","## **Part 2:  Convolutional Neural Networks**\n","---\n","\n","In this section, you will classify the articles using neural networks with `Conv1D` and `MaxPooling1D` hidden layers. Feel free to include `Dense` hidden layers too."],"metadata":{"id":"R-rxKJXJFu-7"}},{"cell_type":"markdown","source":["### **Problem #2.1: Create the highest performing model possible using CNNs**\n","\n","\n","Complete the code below to train a new model that is identical to the one above, except using any or all of the CNN layers that keras provides. The goal is to create a model that performs as well as possible on the *test set*."],"metadata":{"id":"Ek9OyYc_8z6b"}},{"cell_type":"code","source":["model = Sequential()\n","\n","# Input, Vectorization, and Embedding Layers\n","# COMPLETE THIS CODE\n","\n","\n","# Hidden Layers\n","# COMPLETE THIS CODE\n","\n","\n","# Output Layer\n","# COMPLETE THIS CODE\n","\n","\n","# Fitting\n","# COMPLETE THIS CODE\n","\n","\n","# Evaluating\n","print(\"\\n\\n\\n\")\n","# COMPLETE THIS CODE"],"metadata":{"id":"IqhKG-xN8z6h"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Solution**\n"],"metadata":{"id":"t3bZ1mpO8z6h"}},{"cell_type":"code","source":["model = Sequential()\n","\n","# Input, Vectorization, and Embedding Layers\n","model.add(Input(shape=(1,), dtype=tf.string))\n","model.add(vectorize_layer)\n","model.add(Embedding(input_dim = 10000, output_dim = 200, input_length = 50))\n","\n","# Hidden Layers\n","model.add(Conv1D(filters=200, kernel_size=11, activation='relu'))\n","model.add(MaxPooling1D(pool_size=10))\n","\n","model.add(Flatten()) # Add a Flatten layer before the Dense layer\n","model.add(Dense(100, activation='relu'))\n","\n","# Output Layer\n","model.add(Dense(len(dataset['category'].unique()), activation = 'softmax'))\n","\n","# Printing Structure\n","for layer in model.layers:\n","  print(str(layer.input_shape) + \" -> \" + str(layer.output_shape))\n","print(\"\\n\\n\\n\")\n","\n","# Fitting\n","opt = Adam(learning_rate = 0.001)\n","model.compile(optimizer = opt, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n","model.fit(x_train, y_train, epochs = 5, batch_size = 256)\n","\n","# Evaluating\n","print(\"\\n\\n\\n\")\n","model.evaluate(x_train, y_train)\n","model.evaluate(x_test, y_test)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DC2Q36t_xFwv","outputId":"c54e3298-3bc6-48b5-e526-c98d854a498b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(None, 1) -> (None, 20)\n","(None, 20) -> (None, 20, 200)\n","(None, 20, 200) -> (None, 10, 200)\n","(None, 10, 200) -> (None, 1, 200)\n","(None, 1, 200) -> (None, 200)\n","(None, 200) -> (None, 100)\n","(None, 100) -> (None, 5)\n","\n","\n","\n","\n","Epoch 1/5\n","7/7 [==============================] - 7s 50ms/step - loss: 1.5918 - accuracy: 0.2685\n","Epoch 2/5\n","7/7 [==============================] - 0s 14ms/step - loss: 1.4167 - accuracy: 0.6753\n","Epoch 3/5\n","7/7 [==============================] - 0s 14ms/step - loss: 1.0891 - accuracy: 0.9461\n","Epoch 4/5\n","7/7 [==============================] - 0s 14ms/step - loss: 0.6078 - accuracy: 0.9854\n","Epoch 5/5\n","7/7 [==============================] - 0s 14ms/step - loss: 0.2031 - accuracy: 0.9972\n","\n","\n","\n","\n","56/56 [==============================] - 0s 5ms/step - loss: 0.0740 - accuracy: 0.9978\n","14/14 [==============================] - 0s 6ms/step - loss: 0.3363 - accuracy: 0.9169\n"]},{"output_type":"execute_result","data":{"text/plain":["[0.3363010585308075, 0.9168539047241211]"]},"metadata":{},"execution_count":14}]},{"cell_type":"markdown","source":["<a name=\"p3\"></a>\n","\n","---\n","## **Part 3: [OPTIONAL] IMDB Sentiment Classification**\n","---\n","\n","In this part we will focus on building a CNN model using the IMDB sentiment classification dataset. This is a dataset of 25,000 movie reviews with sentiment labels: 0 for negative and 1 for positive.\n","\n","<br>\n","\n","\n","**Run the code provided below to import the dataset.**"],"metadata":{"id":"Tnnv9dfa0I-T"}},{"cell_type":"code","source":["url = 'https://docs.google.com/spreadsheets/d/e/2PACX-1vTdgncgNHtppfS89LHOh1kGl5tYzoEUrUwmOPOQF7mQ0U5Rzba27H45imvZ06_J2x0-wCJySylP5V3_/pub?gid=1712575053&single=true&output=csv'\n","\n","df = pd.read_csv(url)\n","df.head()\n","\n","x_train, x_test, y_train, y_test = train_test_split(df[\"review\"], df[\"sentiment\"], test_size = 0.2, random_state = 42)\n","\n","x_train = np.array(x_train)\n","x_test = np.array(x_test)\n","y_train = to_categorical(y_train)\n","y_test = to_categorical(y_test)"],"metadata":{"id":"ACX4sfJW0bq5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Problem #3.1: Create the `TextVectorization` layer**\n","\n","\n","To get started, let's create a `TextVectorization` layer to vectorize this data.\n","\n","Specifically,\n","1. Initialize the layer with the specified parameters.\n","\n","2. Adapt the layer to the training data.\n","\n","3. Look at the newly built vocabulary."],"metadata":{"id":"HuGSwhDc0e1c"}},{"cell_type":"markdown","source":["#### **1. Initialize the layer with the specified parameters.**\n","\n","* The vocabulary should be at most 5000 words.\n","* The layer's output should always be 64 integers."],"metadata":{"id":"JlbOhj9L0hdD"}},{"cell_type":"code","source":[],"metadata":{"id":"r4CA7F9W0l4S"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### **Solution**"],"metadata":{"id":"p-lLnH1c0lJh"}},{"cell_type":"code","source":["vectorize_layer = TextVectorization(max_tokens = 5000, output_mode = 'int', output_sequence_length = 64)"],"metadata":{"id":"Klz9XvsM0dgG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **2. Adapt the layer to the training data.**"],"metadata":{"id":"3SgPJHmy0plM"}},{"cell_type":"code","source":[],"metadata":{"id":"sGMMXLG50ruI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### **Solution**"],"metadata":{"id":"GXIpwbvTcBUc"}},{"cell_type":"code","source":["vectorize_layer.adapt(x_train)"],"metadata":{"id":"6TY9yKA60p1P"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **3. Look at the newly built vocabulary.**"],"metadata":{"id":"7KWwtA--0ue-"}},{"cell_type":"code","source":[],"metadata":{"id":"t_7qWqMl0xAp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### **Solution**"],"metadata":{"id":"aeUY7Xfd2Fva"}},{"cell_type":"code","source":["vectorize_layer.get_vocabulary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hRY7K9Pt0sdg","outputId":"3e151e8c-26fa-4fbc-c548-fe71b4c41997"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['',\n"," '[UNK]',\n"," 'the',\n"," 'and',\n"," 'a',\n"," 'of',\n"," 'to',\n"," 'is',\n"," 'in',\n"," 'it',\n"," 'i',\n"," 'this',\n"," 'that',\n"," 'br',\n"," 'was',\n"," 'as',\n"," 'for',\n"," 'with',\n"," 'movie',\n"," 'but',\n"," 'film',\n"," 'on',\n"," 'not',\n"," 'you',\n"," 'are',\n"," 'his',\n"," 'have',\n"," 'be',\n"," 'he',\n"," 'one',\n"," 'its',\n"," 'at',\n"," 'all',\n"," 'by',\n"," 'an',\n"," 'they',\n"," 'from',\n"," 'who',\n"," 'so',\n"," 'like',\n"," 'just',\n"," 'or',\n"," 'her',\n"," 'about',\n"," 'if',\n"," 'has',\n"," 'out',\n"," 'some',\n"," 'there',\n"," 'what',\n"," 'good',\n"," 'very',\n"," 'when',\n"," 'more',\n"," 'my',\n"," 'even',\n"," 'no',\n"," 'up',\n"," 'would',\n"," 'she',\n"," 'time',\n"," 'only',\n"," 'which',\n"," 'really',\n"," 'their',\n"," 'see',\n"," 'story',\n"," 'were',\n"," 'had',\n"," 'can',\n"," 'me',\n"," 'we',\n"," 'than',\n"," 'much',\n"," 'well',\n"," 'been',\n"," 'get',\n"," 'do',\n"," 'will',\n"," 'also',\n"," 'great',\n"," 'into',\n"," 'bad',\n"," 'other',\n"," 'people',\n"," 'because',\n"," 'how',\n"," 'most',\n"," 'first',\n"," 'him',\n"," 'dont',\n"," 'then',\n"," 'movies',\n"," 'made',\n"," 'make',\n"," 'could',\n"," 'them',\n"," 'films',\n"," 'way',\n"," 'any',\n"," 'too',\n"," 'after',\n"," 'characters',\n"," 'think',\n"," 'watch',\n"," 'many',\n"," 'seen',\n"," 'being',\n"," 'two',\n"," 'character',\n"," 'never',\n"," 'where',\n"," 'love',\n"," 'acting',\n"," 'plot',\n"," 'did',\n"," 'best',\n"," 'little',\n"," 'know',\n"," 'show',\n"," 'ever',\n"," 'life',\n"," 'does',\n"," 'your',\n"," 'better',\n"," 'off',\n"," 'over',\n"," 'still',\n"," 'end',\n"," 'scene',\n"," 'these',\n"," 'say',\n"," 'while',\n"," 'man',\n"," 'why',\n"," 'scenes',\n"," 'here',\n"," 'such',\n"," 'something',\n"," 'go',\n"," 'should',\n"," 'through',\n"," 'im',\n"," 'those',\n"," 'back',\n"," 'real',\n"," 'watching',\n"," 'thing',\n"," 'doesnt',\n"," 'now',\n"," 'didnt',\n"," 'actors',\n"," 'years',\n"," 'though',\n"," 'actually',\n"," 'another',\n"," 'makes',\n"," 'before',\n"," 'funny',\n"," 'find',\n"," 'nothing',\n"," 'look',\n"," 'going',\n"," 'same',\n"," 'lot',\n"," 'new',\n"," 'every',\n"," 'few',\n"," 'work',\n"," 'old',\n"," 'us',\n"," 'part',\n"," 'again',\n"," 'director',\n"," 'cant',\n"," 'thats',\n"," 'things',\n"," 'quite',\n"," 'want',\n"," 'cast',\n"," 'pretty',\n"," 'seems',\n"," 'around',\n"," 'got',\n"," 'take',\n"," 'young',\n"," 'down',\n"," 'world',\n"," 'fact',\n"," 'however',\n"," 'enough',\n"," 'both',\n"," 'thought',\n"," 'give',\n"," 'big',\n"," 'ive',\n"," 'horror',\n"," 'may',\n"," 'own',\n"," 'without',\n"," 'long',\n"," 'between',\n"," 'saw',\n"," 'isnt',\n"," 'always',\n"," 'original',\n"," 'gets',\n"," 'almost',\n"," 'music',\n"," 'right',\n"," 'must',\n"," 'series',\n"," 'come',\n"," 'whole',\n"," 'least',\n"," 'theres',\n"," 'times',\n"," 'interesting',\n"," 'role',\n"," 'guy',\n"," 'comedy',\n"," 'action',\n"," 'point',\n"," 'done',\n"," 'far',\n"," 'bit',\n"," 'am',\n"," 'feel',\n"," 'might',\n"," 'script',\n"," 'minutes',\n"," 'anything',\n"," 'since',\n"," 'hes',\n"," 'probably',\n"," 'last',\n"," 'family',\n"," 'kind',\n"," 'performance',\n"," 'worst',\n"," 'tv',\n"," 'away',\n"," 'yet',\n"," 'rather',\n"," 'anyone',\n"," 'fun',\n"," 'sure',\n"," 'found',\n"," 'each',\n"," 'making',\n"," 'played',\n"," 'girl',\n"," 'our',\n"," 'although',\n"," 'believe',\n"," 'having',\n"," 'shows',\n"," 'trying',\n"," 'woman',\n"," 'course',\n"," 'hard',\n"," 'especially',\n"," 'comes',\n"," 'put',\n"," 'goes',\n"," 'everything',\n"," 'day',\n"," 'main',\n"," 'worth',\n"," 'maybe',\n"," 'dvd',\n"," 'looking',\n"," 'different',\n"," 'ending',\n"," 'place',\n"," 'watched',\n"," 'looks',\n"," 'once',\n"," 'wasnt',\n"," 'effects',\n"," '2',\n"," 'book',\n"," 'sense',\n"," 'set',\n"," 'reason',\n"," 'three',\n"," 'someone',\n"," 'screen',\n"," 'true',\n"," 'money',\n"," 'during',\n"," 'job',\n"," 'plays',\n"," 'play',\n"," '10',\n"," 'takes',\n"," 'everyone',\n"," 'together',\n"," 'special',\n"," 'said',\n"," 'actor',\n"," 'instead',\n"," 'seem',\n"," 'later',\n"," 'left',\n"," 'seeing',\n"," 'american',\n"," 'beautiful',\n"," 'himself',\n"," 'audience',\n"," 'excellent',\n"," 'john',\n"," 'war',\n"," 'night',\n"," 'version',\n"," 'shot',\n"," 'idea',\n"," 'house',\n"," 'high',\n"," 'fan',\n"," 'simply',\n"," 'black',\n"," 'youre',\n"," 'nice',\n"," 'used',\n"," 'completely',\n"," 'poor',\n"," 'death',\n"," 'read',\n"," 'kids',\n"," 'short',\n"," 'wife',\n"," 'friends',\n"," 'along',\n"," 'second',\n"," 'else',\n"," 'help',\n"," 'try',\n"," 'boring',\n"," 'men',\n"," 'need',\n"," 'given',\n"," 'home',\n"," 'mind',\n"," 'until',\n"," 'star',\n"," 'enjoy',\n"," 'use',\n"," 'half',\n"," 'year',\n"," 'less',\n"," 'either',\n"," 'truly',\n"," 'rest',\n"," 'performances',\n"," 'classic',\n"," 'dead',\n"," 'recommend',\n"," 'production',\n"," 'next',\n"," 'couple',\n"," 'father',\n"," 'tell',\n"," 'wrong',\n"," 'stupid',\n"," 'line',\n"," 'came',\n"," 'let',\n"," 'start',\n"," 'hollywood',\n"," 'full',\n"," 'understand',\n"," 'women',\n"," 'getting',\n"," 'perhaps',\n"," 'terrible',\n"," 'remember',\n"," 'mean',\n"," 'keep',\n"," 'wonderful',\n"," 'camera',\n"," 'school',\n"," 'others',\n"," 'moments',\n"," 'awful',\n"," 'sex',\n"," 'definitely',\n"," 'episode',\n"," 'name',\n"," 'video',\n"," 'human',\n"," 'gives',\n"," 'itself',\n"," 'budget',\n"," 'playing',\n"," 'early',\n"," 'doing',\n"," 'small',\n"," 'often',\n"," 'top',\n"," 'couldnt',\n"," 'person',\n"," 'perfect',\n"," 'finally',\n"," 'dialogue',\n"," 'went',\n"," 'certainly',\n"," 'stars',\n"," 'lines',\n"," 'absolutely',\n"," 'liked',\n"," 'guys',\n"," 'case',\n"," 'piece',\n"," 'face',\n"," 'head',\n"," 'title',\n"," 'sort',\n"," 'hope',\n"," 'loved',\n"," 'become',\n"," 'several',\n"," 'written',\n"," 'entire',\n"," 'supposed',\n"," 'mother',\n"," 'style',\n"," 'felt',\n"," 'lost',\n"," 'overall',\n"," 'live',\n"," 'yes',\n"," 'sound',\n"," '3',\n"," 'worse',\n"," 'entertaining',\n"," 'problem',\n"," 'totally',\n"," 'oh',\n"," 'boy',\n"," 'waste',\n"," 'laugh',\n"," 'based',\n"," 'friend',\n"," 'seemed',\n"," 'picture',\n"," 'against',\n"," 'wanted',\n"," 'mr',\n"," 'beginning',\n"," 'shes',\n"," 'care',\n"," 'youll',\n"," 'dark',\n"," 'cinema',\n"," 'id',\n"," 'direction',\n"," 'final',\n"," 'despite',\n"," 'already',\n"," 'fans',\n"," 'becomes',\n"," 'humor',\n"," 'turn',\n"," 'lead',\n"," 'throughout',\n"," 'evil',\n"," 'example',\n"," 'game',\n"," 'children',\n"," 'wont',\n"," 'lives',\n"," 'low',\n"," 'guess',\n"," 'unfortunately',\n"," 'called',\n"," 'able',\n"," '1',\n"," 'days',\n"," 'girls',\n"," 'wants',\n"," 'drama',\n"," 'fine',\n"," 'quality',\n"," 'under',\n"," 'white',\n"," '\\x96',\n"," 'writing',\n"," 'enjoyed',\n"," 'horrible',\n"," 'history',\n"," 'kill',\n"," 'tries',\n"," 'amazing',\n"," 'michael',\n"," 'gave',\n"," 'theyre',\n"," 'killer',\n"," 'works',\n"," 'turns',\n"," 'act',\n"," 'son',\n"," 'matter',\n"," 'behind',\n"," 'side',\n"," 'expect',\n"," 'favorite',\n"," 'past',\n"," 'flick',\n"," 'brilliant',\n"," 'parts',\n"," 'eyes',\n"," 'obviously',\n"," 'run',\n"," 'themselves',\n"," 'says',\n"," 'ones',\n"," 'killed',\n"," 'viewer',\n"," 'starts',\n"," 'directed',\n"," 'myself',\n"," 'town',\n"," 'car',\n"," 'group',\n"," 'highly',\n"," 'decent',\n"," 'heard',\n"," 'soon',\n"," 'sometimes',\n"," 'thinking',\n"," 'stuff',\n"," 'actress',\n"," 'heart',\n"," 'ill',\n"," 'took',\n"," 'kid',\n"," 'art',\n"," 'city',\n"," 'late',\n"," 'happens',\n"," 'genre',\n"," 'hell',\n"," 'except',\n"," 'leave',\n"," 'child',\n"," 'wouldnt',\n"," 'fight',\n"," 'feeling',\n"," 'extremely',\n"," 'told',\n"," 'cannot',\n"," 'lack',\n"," 'etc',\n"," 'wonder',\n"," 'save',\n"," 'blood',\n"," 'close',\n"," 'including',\n"," 'hour',\n"," 'looked',\n"," 'strong',\n"," 'police',\n"," 'score',\n"," 'ok',\n"," 'shown',\n"," 'particularly',\n"," 'moment',\n"," 'coming',\n"," 'stories',\n"," 'itbr',\n"," 'attempt',\n"," 'obvious',\n"," 'involved',\n"," 'daughter',\n"," 'anyway',\n"," 'hand',\n"," 'experience',\n"," 'living',\n"," 'happen',\n"," 'complete',\n"," 'taken',\n"," 'type',\n"," 'violence',\n"," 'stop',\n"," 'james',\n"," 'chance',\n"," 'happened',\n"," 'simple',\n"," 'serious',\n"," 'god',\n"," 'cool',\n"," 'opening',\n"," 'song',\n"," 'usually',\n"," 'murder',\n"," 'robert',\n"," 'please',\n"," 'across',\n"," 'number',\n"," 'known',\n"," 'roles',\n"," 'david',\n"," 'ago',\n"," 'released',\n"," 'hilarious',\n"," 'voice',\n"," 'hours',\n"," 'order',\n"," 'exactly',\n"," 'huge',\n"," 'english',\n"," 'saying',\n"," 'cinematography',\n"," 'lets',\n"," 'interest',\n"," 'jokes',\n"," 'possible',\n"," 'whose',\n"," 'slow',\n"," 'wish',\n"," 'none',\n"," 'talent',\n"," '5',\n"," 'crap',\n"," 'age',\n"," 'seriously',\n"," 'yourself',\n"," 'started',\n"," 'cut',\n"," 'sad',\n"," 'relationship',\n"," 'running',\n"," 'major',\n"," 'shots',\n"," 'brother',\n"," 'alone',\n"," '4',\n"," 'gore',\n"," 'taking',\n"," 'ridiculous',\n"," 'female',\n"," 'reality',\n"," 'today',\n"," 'somewhat',\n"," 'annoying',\n"," 'beyond',\n"," 'view',\n"," 'hit',\n"," 'call',\n"," 'ends',\n"," 'change',\n"," 'career',\n"," 'knew',\n"," 'important',\n"," 'documentary',\n"," 'turned',\n"," 'strange',\n"," 'body',\n"," 'power',\n"," 'scary',\n"," 'hero',\n"," 'basically',\n"," 'usual',\n"," 'mostly',\n"," 'apparently',\n"," 'episodes',\n"," 'opinion',\n"," 'finds',\n"," 'word',\n"," 'silly',\n"," 'words',\n"," 'upon',\n"," 'husband',\n"," 'knows',\n"," 'rating',\n"," 'due',\n"," 'talking',\n"," 'novel',\n"," 'four',\n"," 'whats',\n"," 'british',\n"," 'country',\n"," 'happy',\n"," 'problems',\n"," 'clearly',\n"," 'attention',\n"," 'moviebr',\n"," 'arent',\n"," 'single',\n"," 'musical',\n"," 'directors',\n"," 'local',\n"," 'miss',\n"," 'add',\n"," 'room',\n"," 'disappointed',\n"," 'jack',\n"," 'tells',\n"," 'level',\n"," 'events',\n"," 'talk',\n"," 'light',\n"," 'cheap',\n"," 'modern',\n"," 'television',\n"," 'songs',\n"," 'sequence',\n"," 'predictable',\n"," 'whether',\n"," 'review',\n"," 'appears',\n"," 'supporting',\n"," 'easily',\n"," 'dialog',\n"," 'sets',\n"," 'lots',\n"," 'falls',\n"," 'giving',\n"," 'five',\n"," 'havent',\n"," 'soundtrack',\n"," 'ten',\n"," 'tried',\n"," 'space',\n"," 'similar',\n"," 'moving',\n"," 'mention',\n"," 'bring',\n"," 'above',\n"," 'within',\n"," 'team',\n"," 'needs',\n"," 'enjoyable',\n"," 'parents',\n"," 'future',\n"," 'certain',\n"," 'french',\n"," 'viewers',\n"," 'bunch',\n"," 'romantic',\n"," 'entertainment',\n"," 'storyline',\n"," 'filmbr',\n"," 'showing',\n"," 'hate',\n"," 'surprised',\n"," 'filmed',\n"," 'dull',\n"," 'middle',\n"," 'george',\n"," 'among',\n"," 'clear',\n"," 'theme',\n"," 'earth',\n"," 'subject',\n"," 'stay',\n"," 'message',\n"," 'thriller',\n"," 'named',\n"," 'fall',\n"," 'using',\n"," 'typical',\n"," 'sorry',\n"," 'king',\n"," 'sequel',\n"," 'easy',\n"," 'comments',\n"," 'greatest',\n"," 'feels',\n"," 'nearly',\n"," 'kept',\n"," 'release',\n"," 'working',\n"," 'theater',\n"," 'ways',\n"," 'elements',\n"," 'effort',\n"," 'buy',\n"," 'near',\n"," 'deal',\n"," 'tale',\n"," 'writer',\n"," 'means',\n"," 'avoid',\n"," 'brought',\n"," 'doubt',\n"," 'class',\n"," 'youve',\n"," 'herself',\n"," 'actual',\n"," 'straight',\n"," 'realistic',\n"," 'boys',\n"," 'suspense',\n"," 'editing',\n"," 'famous',\n"," 'sister',\n"," 'comic',\n"," '80s',\n"," 'gone',\n"," 'richard',\n"," 'rock',\n"," 'feature',\n"," 'reviews',\n"," 'learn',\n"," 'dr',\n"," 'die',\n"," 'lady',\n"," 'points',\n"," 'general',\n"," 'leads',\n"," 'imagine',\n"," 'move',\n"," 'check',\n"," 'peter',\n"," 'somehow',\n"," 'monster',\n"," 'begins',\n"," 'fantastic',\n"," 'material',\n"," 'whos',\n"," 'hear',\n"," 'forget',\n"," 'particular',\n"," 'okay',\n"," 'viewing',\n"," 'weak',\n"," 'form',\n"," 'crime',\n"," 'animation',\n"," 'mystery',\n"," 'sequences',\n"," 'figure',\n"," 'rent',\n"," 'decided',\n"," 'deep',\n"," 'premise',\n"," 'sit',\n"," 'killing',\n"," 'eventually',\n"," 'tom',\n"," 'fast',\n"," 'atmosphere',\n"," 'period',\n"," 'surprise',\n"," 'paul',\n"," 'believable',\n"," 'emotional',\n"," 'forced',\n"," 'shame',\n"," 'expected',\n"," 'red',\n"," 'truth',\n"," 'dog',\n"," 'poorly',\n"," 'follow',\n"," 'difficult',\n"," 'indeed',\n"," 'eye',\n"," 'wait',\n"," 'crew',\n"," 'became',\n"," 'scifi',\n"," 'whatever',\n"," 'memorable',\n"," 'york',\n"," 'lame',\n"," 'average',\n"," 'season',\n"," 'leaves',\n"," 'doctor',\n"," 'possibly',\n"," 'situation',\n"," 'dance',\n"," 'stand',\n"," 'needed',\n"," 'open',\n"," 'interested',\n"," 'cheesy',\n"," 'superb',\n"," 'meets',\n"," 'credits',\n"," 'reading',\n"," 'unless',\n"," 'note',\n"," 'begin',\n"," 'sexual',\n"," 'nature',\n"," 'filmmakers',\n"," 'footage',\n"," 'write',\n"," 'imdb',\n"," 'writers',\n"," 'otherwise',\n"," 'western',\n"," 'third',\n"," 'incredibly',\n"," 'plus',\n"," 'question',\n"," 'features',\n"," 'meet',\n"," 'gay',\n"," 'minute',\n"," 'male',\n"," 'towards',\n"," 'hands',\n"," 'romance',\n"," 'earlier',\n"," 'beauty',\n"," 'oscar',\n"," 'nor',\n"," 'whom',\n"," 'personal',\n"," 'previous',\n"," 'keeps',\n"," 'inside',\n"," 'quickly',\n"," 'box',\n"," 'society',\n"," 'hot',\n"," 'sounds',\n"," 'copy',\n"," '20',\n"," 'unique',\n"," 'result',\n"," 'total',\n"," 'badly',\n"," 'screenplay',\n"," 'realize',\n"," 'setting',\n"," 'weird',\n"," 'battle',\n"," 'japanese',\n"," 'background',\n"," 'laughs',\n"," 'effect',\n"," 'comment',\n"," 'following',\n"," 'ask',\n"," 'powerful',\n"," 'directing',\n"," 'appear',\n"," 'various',\n"," 'worked',\n"," 'older',\n"," 'america',\n"," 'masterpiece',\n"," 'street',\n"," 'island',\n"," 'air',\n"," 'free',\n"," 'portrayed',\n"," 'perfectly',\n"," 'de',\n"," 'lee',\n"," 'plenty',\n"," 'development',\n"," 'brings',\n"," 'water',\n"," 'joe',\n"," 'b',\n"," 'admit',\n"," 'acted',\n"," 'stage',\n"," 'mark',\n"," 'outside',\n"," 'fairly',\n"," 'spent',\n"," 'forward',\n"," 'dumb',\n"," 'meant',\n"," 'baby',\n"," 'front',\n"," 'deserves',\n"," 'attempts',\n"," 'remake',\n"," 'creepy',\n"," 'create',\n"," 'manages',\n"," 'apart',\n"," '70s',\n"," 'expecting',\n"," 'bill',\n"," 'business',\n"," 'agree',\n"," 'leading',\n"," 'joke',\n"," 'wasted',\n"," 'hardly',\n"," 'dramatic',\n"," 'cover',\n"," 'caught',\n"," 'crazy',\n"," 'reasons',\n"," 'ideas',\n"," 'fails',\n"," 'fire',\n"," 'brothers',\n"," 'present',\n"," ...]"]},"metadata":{},"execution_count":48}]},{"cell_type":"markdown","source":["### **Problem #3.2: Build and Train a Dense model**\n","\n","Complete the code below to build a model with the following layers.\n","\n","An Embedding layer such that:\n","- The vocabulary contains 5000 tokens.\n","- The input length corresponds to the output of the vectorization layer.\n","- The number of outputs per input is 128.\n","\n","<br>\n","\n","Hidden layers such that:\n","\n","- There's at least one Dense layer.\n","\n","<br>\n","\n","A Dense layer for outputting classification probabilities for \"negative\" or \"positive\" labels."],"metadata":{"id":"XH9dVdBW1D4w"}},{"cell_type":"code","source":[],"metadata":{"id":"tr-vmrCI1_mU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Solution**"],"metadata":{"id":"wBTMxSAY1_3K"}},{"cell_type":"code","source":["model = Sequential()\n","\n","# Input, Vectorization, and Embedding Layers\n","model.add(Input(shape=(1,), dtype=tf.string))\n","model.add(vectorize_layer)\n","model.add(Embedding(input_dim = 5000, output_dim = 128, input_length = 64))\n","\n","# Hidden Layers\n","model.add(Flatten()) # Add a Flatten layer before the Dense layer\n","model.add(Dense(128, activation='relu'))\n","\n","# Output Layer\n","model.add(Dense(2, activation = 'softmax'))\n","\n","# Printing Structure\n","for layer in model.layers:\n","  print(str(layer.input_shape) + \" -> \" + str(layer.output_shape))\n","print(\"\\n\\n\\n\")\n","\n","# Fitting\n","opt = Adam(learning_rate = 0.001)\n","model.compile(optimizer = opt, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n","model.fit(x_train, y_train, epochs = 5, batch_size = 256)\n","\n","# Evaluating\n","print(\"\\n\\n\\n\")\n","model.evaluate(x_train, y_train)\n","model.evaluate(x_test, y_test)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5PtnopbD1EXo","outputId":"c41e4a5c-39f1-4757-d248-ede186b7c82c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(None, 1) -> (None, 64)\n","(None, 64) -> (None, 64, 128)\n","(None, 64, 128) -> (None, 8192)\n","(None, 8192) -> (None, 128)\n","(None, 128) -> (None, 2)\n","\n","\n","\n","\n","Epoch 1/5\n","157/157 [==============================] - 3s 15ms/step - loss: 0.5260 - accuracy: 0.7175\n","Epoch 2/5\n","157/157 [==============================] - 2s 15ms/step - loss: 0.2715 - accuracy: 0.8884\n","Epoch 3/5\n","157/157 [==============================] - 2s 15ms/step - loss: 0.0631 - accuracy: 0.9808\n","Epoch 4/5\n","157/157 [==============================] - 3s 20ms/step - loss: 0.0093 - accuracy: 0.9986\n","Epoch 5/5\n","157/157 [==============================] - 2s 15ms/step - loss: 0.0012 - accuracy: 1.0000\n","\n","\n","\n","\n","1250/1250 [==============================] - 7s 5ms/step - loss: 5.2351e-04 - accuracy: 1.0000\n","313/313 [==============================] - 1s 4ms/step - loss: 1.1797 - accuracy: 0.7589\n"]},{"output_type":"execute_result","data":{"text/plain":["[1.1797369718551636, 0.758899986743927]"]},"metadata":{},"execution_count":49}]},{"cell_type":"markdown","source":["This other alternative includes building the model with CNN.\n","**Which architecture performs better?**"],"metadata":{"id":"_UHdf2m15v1u"}},{"cell_type":"code","source":["# [OPTIONAL] USING CNNs\n","model = Sequential()\n","\n","# Input, Vectorization, and Embedding Layers\n","model.add(Input(shape=(1,), dtype=tf.string))\n","model.add(vectorize_layer)\n","model.add(Embedding(input_dim = 5000, output_dim = 128, input_length = 64))\n","\n","# Hidden Layers\n","model.add(Conv1D(filters = 16, kernel_size = 4, activation = 'relu'))\n","model.add(MaxPooling1D(pool_size = 3))\n","model.add(Flatten())\n","\n","# Output Layer\n","model.add(Dense(2, activation = 'softmax'))\n","\n","\n","\n","# Printing Structure\n","for layer in model.layers:\n","  print(str(layer.input_shape) + \" -> \" + str(layer.output_shape))\n","print(\"\\n\\n\\n\")\n","\n","\n","\n","# Fitting\n","opt = Adam(learning_rate = 0.001)\n","model.compile(optimizer = opt, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n","model.fit(x_train, y_train, epochs = 5, batch_size = 256)\n","\n","\n","# Evaluating\n","print(\"\\n\\n\\n\")\n","model.evaluate(x_train, y_train)\n","model.evaluate(x_test, y_test)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LSM9qX412zyL","outputId":"de7fb0cf-a6b0-4ad8-d0af-3671ca00cfcb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(None, 1) -> (None, 64)\n","(None, 64) -> (None, 64, 128)\n","(None, 64, 128) -> (None, 61, 16)\n","(None, 61, 16) -> (None, 20, 16)\n","(None, 20, 16) -> (None, 320)\n","(None, 320) -> (None, 2)\n","\n","\n","\n","\n","Epoch 1/5\n","157/157 [==============================] - 6s 26ms/step - loss: 0.5786 - accuracy: 0.6819\n","Epoch 2/5\n","157/157 [==============================] - 3s 17ms/step - loss: 0.3879 - accuracy: 0.8229\n","Epoch 3/5\n","157/157 [==============================] - 3s 21ms/step - loss: 0.3235 - accuracy: 0.8619\n","Epoch 4/5\n","157/157 [==============================] - 3s 16ms/step - loss: 0.2545 - accuracy: 0.9027\n","Epoch 5/5\n","157/157 [==============================] - 3s 21ms/step - loss: 0.1674 - accuracy: 0.9501\n","\n","\n","\n","\n","1250/1250 [==============================] - 8s 6ms/step - loss: 0.0919 - accuracy: 0.9890\n","313/313 [==============================] - 2s 6ms/step - loss: 0.5065 - accuracy: 0.7865\n"]},{"output_type":"execute_result","data":{"text/plain":["[0.5064528584480286, 0.7864999771118164]"]},"metadata":{},"execution_count":51}]},{"cell_type":"markdown","source":["---\n","###© 2024 The Coding School, All rights reserved"],"metadata":{"id":"8zFgD9Jx1ooV"}}]}