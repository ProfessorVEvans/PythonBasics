{"cells":[{"cell_type":"markdown","source":["# **Homework 21: Natural Language Processing II**\n","---\n","\n","### **Description**\n","In this week's homework, you will see how to use more advanced forms of neural nets to perform tasks in NLP such as classification and generation.\n","\n","<br>\n","\n","### **Structure**\n","**Part 1**: Sarcasm Detection in the News\n","\n","\n","\n","\n","<br>\n","\n","### **Cheat Sheets**\n","[Natural Language Processing II](https://docs.google.com/document/d/1p3xVUL1F6SEkusCI4klPLYqQwCkVN5s00ZvJjBpiSqM/edit?usp=sharing)\n","\n","<br>\n","\n","**Before starting, run the code below to import all necessary functions and libraries.**"],"metadata":{"id":"dTIoQdGufbcb"}},{"cell_type":"code","source":["!pip install lime\n","\n","from lime import lime_text\n","import numpy as np\n","import pandas as pd\n","\n","import tensorflow as tf\n","import numpy as np\n","import os\n","\n","from keras.models import Sequential\n","from keras.layers import *\n","from keras.optimizers import Adam, SGD\n","from keras.utils import to_categorical\n","\n","from sklearn.model_selection import train_test_split\n","\n","from random import choices\n","\n","import warnings\n","warnings.filterwarnings('ignore')"],"metadata":{"id":"yWd0JEE9fmrW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1704489687812,"user_tz":480,"elapsed":19202,"user":{"displayName":"Angelique Membrido","userId":"04984385250682432912"}},"outputId":"6956b67c-0400-4200-dff0-614a22955283"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting lime\n","  Downloading lime-0.2.0.1.tar.gz (275 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m275.7/275.7 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from lime) (3.7.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from lime) (1.23.5)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from lime) (1.11.4)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from lime) (4.66.1)\n","Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.10/dist-packages (from lime) (1.2.2)\n","Requirement already satisfied: scikit-image>=0.12 in /usr/local/lib/python3.10/dist-packages (from lime) (0.19.3)\n","Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.12->lime) (3.2.1)\n","Requirement already satisfied: pillow!=7.1.0,!=7.1.1,!=8.3.0,>=6.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.12->lime) (9.4.0)\n","Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.12->lime) (2.31.6)\n","Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.12->lime) (2023.12.9)\n","Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.12->lime) (1.5.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.12->lime) (23.2)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.18->lime) (1.3.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.18->lime) (3.2.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lime) (1.2.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lime) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lime) (4.47.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lime) (1.4.5)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lime) (3.1.1)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lime) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->lime) (1.16.0)\n","Building wheels for collected packages: lime\n","  Building wheel for lime (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for lime: filename=lime-0.2.0.1-py3-none-any.whl size=283835 sha256=46481b7ae74bdc6fb2b192104840cf6526c282f5f31834180104fcd7a0fb2287\n","  Stored in directory: /root/.cache/pip/wheels/fd/a2/af/9ac0a1a85a27f314a06b39e1f492bee1547d52549a4606ed89\n","Successfully built lime\n","Installing collected packages: lime\n","Successfully installed lime-0.2.0.1\n"]}]},{"cell_type":"markdown","source":["---\n","## **Part 1: Sarcasm Detection in the News**\n","---\n","\n","In this section, we will see how to apply what we learned yesterday in combination with more advanced tools to determine how sarcastic a given text is.\n","\n","<br>\n","\n","We will be working with a dataset containing the headline of many news articles and a classification of whether that headline is sarcastic or not.\n","\n","<br>\n","\n","\n","**Run the code provided below to import the dataset.**"],"metadata":{"id":"iSkUvAmLfB78"}},{"cell_type":"code","source":["data = pd.read_csv('https://docs.google.com/spreadsheets/d/e/2PACX-1vTHrKLcHxF-DSkeH5FMmpFm5KQzDbzdaCdj1aP89wmUVIg_TxLPaveVXt1C8kG2b3aLnuON6cqfABd5/pub?output=csv')\n","data.head()\n","\n","x_train, x_test, y_train, y_test = train_test_split(data[\"headline\"], data[\"is_sarcastic\"], test_size = 0.2, random_state = 42)\n","\n","x_train = np.array(x_train)\n","x_test = np.array(x_test)\n","\n","y_train = to_categorical(y_train)\n","y_test = to_categorical(y_test)"],"metadata":{"id":"wLXOcKuBfod0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","### **Part 1.1:  Simple Deep Neural Networks**\n","---\n","\n","In this section, you will use dense layers provided by keras to build a simple DNN."],"metadata":{"id":"OgvzN47sq0oa"}},{"cell_type":"markdown","source":["#### **Problem #1.1.1: Create the `TextVectorization` layer**\n","\n","\n","To get started, let's create a `TextVectorization` layer to vectorize this data.\n","\n","Specifically,\n","1. Initialize the layer with the specified parameters.\n","\n","2. Adapt the layer to the training data.\n","\n","3. Look at the newly built vocabulary."],"metadata":{"id":"LTm3147qiKE4"}},{"cell_type":"markdown","source":["##### **1.Initialize the layer with the specified parameters.**\n","\n","* The vocabulary should be at most 2000 words.\n","* The layer's output should always be 20 integers."],"metadata":{"id":"d3zfKfgAiKE4"}},{"cell_type":"code","source":[],"metadata":{"id":"Lc6-HJGhiKE4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###### **Solution**\n"],"metadata":{"id":"GS782-N7iKE4"}},{"cell_type":"code","source":["vectorize_layer = TextVectorization(max_tokens = 2000, output_mode = 'int', output_sequence_length = 20)"],"metadata":{"id":"b-kOeNYRiKE5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### **2. Adapt the layer to the training data.**"],"metadata":{"id":"Bif_POk3iKE5"}},{"cell_type":"code","source":[],"metadata":{"id":"bsnDnHGFiKE5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###### **Solution**\n"],"metadata":{"id":"jJRRdYCuiKE5"}},{"cell_type":"code","source":["vectorize_layer.adapt(x_train)"],"metadata":{"id":"jIOkTtuniKE5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### **3. Look at the newly built vocabulary.**"],"metadata":{"id":"ovHm-Icy2iZv"}},{"cell_type":"code","source":[],"metadata":{"id":"bnAtuO3B2iZw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###### **Solution**\n"],"metadata":{"id":"fQ4o2Bkc2iZx"}},{"cell_type":"code","source":["vectorize_layer.get_vocabulary()"],"metadata":{"executionInfo":{"status":"ok","timestamp":1679292052781,"user_tz":-60,"elapsed":11,"user":{"displayName":"Eranda Bregasi","userId":"06402694060653755536"}},"colab":{"base_uri":"https://localhost:8080/"},"id":"lAvM6ntr2iZx","outputId":"bf0f6a16-3f72-49fc-ae70-c87c62d48eff"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['',\n"," '[UNK]',\n"," 'to',\n"," 'of',\n"," 'the',\n"," 'in',\n"," 'for',\n"," 'a',\n"," 'on',\n"," 'and',\n"," 'with',\n"," 'is',\n"," 'new',\n"," 'trump',\n"," 'man',\n"," 'from',\n"," 'at',\n"," 'about',\n"," 'you',\n"," 'this',\n"," 'by',\n"," 'after',\n"," 'be',\n"," 'how',\n"," 'out',\n"," 'it',\n"," 'up',\n"," 'that',\n"," 'as',\n"," 'not',\n"," 'your',\n"," 'his',\n"," 'are',\n"," 'what',\n"," 'he',\n"," 'just',\n"," 'us',\n"," 'has',\n"," 'who',\n"," 'more',\n"," 'will',\n"," 'all',\n"," 'one',\n"," 'into',\n"," 'report',\n"," 'why',\n"," 'have',\n"," 'donald',\n"," 'area',\n"," 'over',\n"," 'can',\n"," 'woman',\n"," 'says',\n"," 'its',\n"," 'day',\n"," 'time',\n"," 'trumps',\n"," 'no',\n"," 'her',\n"," 'like',\n"," 'get',\n"," 'an',\n"," 'first',\n"," 'people',\n"," 'now',\n"," 'obama',\n"," 'i',\n"," 'off',\n"," 'make',\n"," 'life',\n"," 'was',\n"," 'than',\n"," 'house',\n"," 'still',\n"," 'we',\n"," 'if',\n"," 'my',\n"," 'back',\n"," 'white',\n"," 'when',\n"," 'women',\n"," 'down',\n"," 'clinton',\n"," 'world',\n"," 'could',\n"," 'years',\n"," 'americans',\n"," 'family',\n"," 'before',\n"," 'most',\n"," 'do',\n"," 'black',\n"," 'way',\n"," 'their',\n"," 'study',\n"," '5',\n"," 'would',\n"," 'they',\n"," 'gop',\n"," 'only',\n"," 'should',\n"," 'really',\n"," 'being',\n"," 'show',\n"," 'best',\n"," 'so',\n"," 'police',\n"," 'cant',\n"," 'him',\n"," 'watch',\n"," 'school',\n"," 'bill',\n"," 'good',\n"," 'going',\n"," 'nation',\n"," 'during',\n"," 'but',\n"," 'home',\n"," 'know',\n"," 'american',\n"," 'death',\n"," 'finds',\n"," 'health',\n"," 'president',\n"," 'things',\n"," 'say',\n"," 'parents',\n"," 'last',\n"," 'mom',\n"," 'year',\n"," 'video',\n"," 'state',\n"," 'or',\n"," 'kids',\n"," 'too',\n"," 'love',\n"," 'may',\n"," 'every',\n"," 'she',\n"," 'big',\n"," 'against',\n"," 'hillary',\n"," 'getting',\n"," 'gets',\n"," 'campaign',\n"," 'right',\n"," 'these',\n"," 'party',\n"," 'need',\n"," 'america',\n"," 'other',\n"," 'dead',\n"," '3',\n"," 'our',\n"," 'makes',\n"," 'some',\n"," 'change',\n"," '10',\n"," 'nations',\n"," 'little',\n"," 'court',\n"," 'take',\n"," 'own',\n"," 'john',\n"," 'heres',\n"," 'go',\n"," 'calls',\n"," 'takes',\n"," 'dont',\n"," 'while',\n"," 'work',\n"," 'where',\n"," 'want',\n"," 'see',\n"," 'even',\n"," 'through',\n"," 'stop',\n"," 'doesnt',\n"," 'child',\n"," 'plan',\n"," 'local',\n"," 'real',\n"," 'next',\n"," 'news',\n"," 'never',\n"," 'hes',\n"," 'guy',\n"," 'college',\n"," 'war',\n"," 'gay',\n"," 'look',\n"," 'again',\n"," 'dad',\n"," 'bush',\n"," 'election',\n"," 'help',\n"," 'another',\n"," 'office',\n"," '2',\n"," 'thing',\n"," 'dog',\n"," 'star',\n"," 'north',\n"," 'made',\n"," 'actually',\n"," 'wants',\n"," 'got',\n"," 'much',\n"," 'debate',\n"," 'care',\n"," 'baby',\n"," 'week',\n"," 'gun',\n"," 'been',\n"," 'game',\n"," 'them',\n"," 'live',\n"," 'finally',\n"," 'congress',\n"," 'job',\n"," 'friends',\n"," 'ever',\n"," 'bad',\n"," 'around',\n"," 'national',\n"," 'ways',\n"," 'million',\n"," 'high',\n"," 'away',\n"," 'announces',\n"," 'teen',\n"," 'season',\n"," 'better',\n"," 'wont',\n"," 'top',\n"," 'money',\n"," '7',\n"," 'me',\n"," 'facebook',\n"," '6',\n"," 'students',\n"," 'sex',\n"," 'food',\n"," 'under',\n"," 'story',\n"," 'mans',\n"," 'were',\n"," 'old',\n"," 'media',\n"," 'without',\n"," 'two',\n"," 'there',\n"," 'sexual',\n"," 'night',\n"," 'end',\n"," 'couple',\n"," 'children',\n"," 'men',\n"," 'had',\n"," 'give',\n"," 'any',\n"," '4',\n"," 'trying',\n"," 'shows',\n"," 'paul',\n"," 'fight',\n"," 'climate',\n"," 'supreme',\n"," 'shooting',\n"," 'senate',\n"," 'reveals',\n"," 'face',\n"," 'enough',\n"," 'history',\n"," 'wedding',\n"," 'everyone',\n"," 'attack',\n"," 'law',\n"," 'deal',\n"," 'god',\n"," 'girl',\n"," 'york',\n"," 'photos',\n"," 'movie',\n"," 'making',\n"," 'fire',\n"," 'does',\n"," 'book',\n"," 'self',\n"," 'found',\n"," 'introduces',\n"," 'government',\n"," 'city',\n"," 'worlds',\n"," 'great',\n"," 'former',\n"," 'find',\n"," 'email',\n"," 'car',\n"," 'pretty',\n"," 'power',\n"," 'part',\n"," 'democrats',\n"," 'son',\n"," 'pope',\n"," 'film',\n"," 'business',\n"," 'tell',\n"," 'republicans',\n"," 'entire',\n"," 'come',\n"," 'call',\n"," 'use',\n"," 'tv',\n"," 'thinks',\n"," 'speech',\n"," 'sanders',\n"," 'republican',\n"," 'presidential',\n"," 'keep',\n"," 'coming',\n"," 'body',\n"," 'very',\n"," 'support',\n"," 'run',\n"," 'name',\n"," 'future',\n"," 'behind',\n"," 'think',\n"," 'talk',\n"," 'having',\n"," 'group',\n"," 'friend',\n"," 'company',\n"," '8',\n"," 'single',\n"," 'morning',\n"," 'might',\n"," 'case',\n"," 'asks',\n"," 'james',\n"," 'christmas',\n"," 'doing',\n"," 'didnt',\n"," '20',\n"," 'vote',\n"," 'used',\n"," 'tax',\n"," 'states',\n"," 'killed',\n"," 'fans',\n"," 'already',\n"," 'rights',\n"," 'race',\n"," 'public',\n"," 'minutes',\n"," 'goes',\n"," 'between',\n"," 'super',\n"," 'looking',\n"," 'line',\n"," 'free',\n"," 'department',\n"," 'ban',\n"," 'twitter',\n"," 'times',\n"," 'save',\n"," 'releases',\n"," 'perfect',\n"," 'once',\n"," 'middle',\n"," 'long',\n"," 'girls',\n"," '2016',\n"," '12',\n"," 'room',\n"," 'each',\n"," 'security',\n"," 'plans',\n"," 'must',\n"," 'music',\n"," 'missing',\n"," 'marriage',\n"," 'control',\n"," 'claims',\n"," 'boy',\n"," 'violence',\n"," 'student',\n"," 'same',\n"," 'past',\n"," 'living',\n"," 'inside',\n"," 'ad',\n"," 'wife',\n"," 'teacher',\n"," 'secret',\n"," 'poll',\n"," 'win',\n"," 'tells',\n"," 'team',\n"," 'sure',\n"," 'something',\n"," 'second',\n"," 'put',\n"," 'political',\n"," 'photo',\n"," 'lets',\n"," 'let',\n"," 'forced',\n"," 'bernie',\n"," 'voters',\n"," 'reports',\n"," 'person',\n"," 'obamacare',\n"," 'mike',\n"," 'im',\n"," 'human',\n"," 'here',\n"," 'country',\n"," 'art',\n"," 'water',\n"," 'until',\n"," 'texas',\n"," 'shot',\n"," 'ready',\n"," 'open',\n"," 'list',\n"," 'full',\n"," 'employee',\n"," 'biden',\n"," 'always',\n"," 'admits',\n"," 'talks',\n"," 'summer',\n"," 'start',\n"," 'ryan',\n"," 'needs',\n"," 'korea',\n"," 'judge',\n"," 'female',\n"," 'dies',\n"," '1',\n"," 'womens',\n"," 'unveils',\n"," 'red',\n"," 'michael',\n"," 'meet',\n"," 'everything',\n"," 'days',\n"," 'youre',\n"," 'young',\n"," 'stars',\n"," 'social',\n"," 'running',\n"," 'percent',\n"," 'because',\n"," 'today',\n"," 'scientists',\n"," 'record',\n"," 'many',\n"," 'comes',\n"," 'attacks',\n"," 'wrong',\n"," 'taking',\n"," 'south',\n"," 'looks',\n"," 'left',\n"," 'hot',\n"," 'daughter',\n"," 'cruz',\n"," 'wall',\n"," 'tweets',\n"," 'thousands',\n"," 'thought',\n"," 'shes',\n"," 'romney',\n"," 'probably',\n"," 'place',\n"," 'meeting',\n"," 'lives',\n"," 'head',\n"," 'fucking',\n"," 'cancer',\n"," 'working',\n"," 'whats',\n"," 'washington',\n"," 'warns',\n"," 'three',\n"," 'set',\n"," 'secretary',\n"," 'owner',\n"," 'mother',\n"," 'lost',\n"," 'idea',\n"," 'hours',\n"," 'did',\n"," 'crisis',\n"," 'town',\n"," 'street',\n"," 'russia',\n"," 'pay',\n"," 'obamas',\n"," 'moms',\n"," 'kim',\n"," 'justice',\n"," 'guide',\n"," 'george',\n"," 'feel',\n"," 'candidate',\n"," 'california',\n"," 'age',\n"," 'tips',\n"," 'someone',\n"," 'military',\n"," 'leaves',\n"," 'kill',\n"," 'internet',\n"," 'gives',\n"," 'fbi',\n"," 'education',\n"," 'class',\n"," 'chief',\n"," 'cat',\n"," 'yet',\n"," 'ted',\n"," 'sleep',\n"," 'online',\n"," 'months',\n"," 'interview',\n"," 'immigration',\n"," 'hollywood',\n"," 'heart',\n"," 'giving',\n"," 'few',\n"," 'father',\n"," 'eating',\n"," 'thinking',\n"," 'restaurant',\n"," 'order',\n"," 'officials',\n"," 'move',\n"," 'mark',\n"," 'march',\n"," 'king',\n"," 'isis',\n"," 'iran',\n"," 'federal',\n"," 'fan',\n"," 'democratic',\n"," 'ceo',\n"," 'administration',\n"," 'wins',\n"," 'using',\n"," 'together',\n"," 'talking',\n"," 'shit',\n"," 'service',\n"," 'prison',\n"," 'phone',\n"," 'month',\n"," 'latest',\n"," 'holiday',\n"," 'earth',\n"," 'air',\n"," 'travel',\n"," 'series',\n"," 'relationship',\n"," 'personal',\n"," 'nuclear',\n"," 'moment',\n"," 'letter',\n"," 'leaders',\n"," 'knows',\n"," 'kind',\n"," 'chris',\n"," 'breaking',\n"," 'believe',\n"," 'beautiful',\n"," 'those',\n"," 'stephen',\n"," 'since',\n"," 'rules',\n"," 'rock',\n"," 'reason',\n"," 'read',\n"," 'outside',\n"," 'nothing',\n"," 'himself',\n"," 'hair',\n"," 'director',\n"," 'cover',\n"," '2015',\n"," 'watching',\n"," 'visit',\n"," 'union',\n"," 'response',\n"," 'problem',\n"," 'message',\n"," 'less',\n"," 'issues',\n"," 'happy',\n"," 'florida',\n"," 'favorite',\n"," 'excited',\n"," 'dream',\n"," 'different',\n"," 'cops',\n"," 'awards',\n"," 'well',\n"," 'victims',\n"," 'united',\n"," 'totally',\n"," 'third',\n"," 'straight',\n"," 'special',\n"," 'kid',\n"," 'isnt',\n"," 'francis',\n"," 'drug',\n"," 'community',\n"," 'birthday',\n"," 'bar',\n"," '911',\n"," 'weekend',\n"," 'waiting',\n"," 'small',\n"," 'schools',\n"," 'reasons',\n"," 'questions',\n"," 'protest',\n"," 'lot',\n"," 'jimmy',\n"," 'investigation',\n"," 'hope',\n"," 'hell',\n"," 'david',\n"," 'candidates',\n"," 'buy',\n"," 'bring',\n"," 'box',\n"," 'assault',\n"," 'americas',\n"," 'workers',\n"," 'queer',\n"," 'least',\n"," 'joe',\n"," 'front',\n"," '9',\n"," 'vows',\n"," 'un',\n"," 'trailer',\n"," 'theres',\n"," 'senator',\n"," 'rise',\n"," 'powerful',\n"," 'lessons',\n"," 'leave',\n"," 'learned',\n"," 'ice',\n"," 'huge',\n"," 'hits',\n"," 'hard',\n"," 'girlfriend',\n"," 'fox',\n"," 'following',\n"," 'dinner',\n"," 'die',\n"," 'cop',\n"," 'conversation',\n"," 'congressman',\n"," 'celebrates',\n"," 'birth',\n"," 'become',\n"," '50',\n"," '30',\n"," '15',\n"," '11',\n"," 'turn',\n"," 'told',\n"," 'taylor',\n"," 'syria',\n"," 'signs',\n"," 'returns',\n"," 'point',\n"," 'play',\n"," 'offers',\n"," 'millions',\n"," 'majority',\n"," 'leader',\n"," 'hit',\n"," 'drunk',\n"," 'chinese',\n"," 'career',\n"," 'ask',\n"," 'anything',\n"," 'advice',\n"," 'adorable',\n"," 'whole',\n"," 'trip',\n"," 'tom',\n"," 'syrian',\n"," 'stage',\n"," 'scott',\n"," 'program',\n"," 'politics',\n"," 'pence',\n"," 'mothers',\n"," 'members',\n"," 'massive',\n"," 'global',\n"," 'gift',\n"," 'feels',\n"," 'crash',\n"," 'completely',\n"," 'words',\n"," 'word',\n"," 'walking',\n"," 'turns',\n"," 'sports',\n"," 'song',\n"," 'reality',\n"," 'puts',\n"," 'muslim',\n"," 'late',\n"," 'kills',\n"," 'host',\n"," 'hands',\n"," 'fashion',\n"," 'fall',\n"," 'experience',\n"," 'early',\n"," 'date',\n"," 'called',\n"," 'break',\n"," 'boys',\n"," 'band',\n"," 'accused',\n"," 'abortion',\n"," 'theyre',\n"," 'sick',\n"," 'post',\n"," 'paris',\n"," 'mental',\n"," 'fun',\n"," 'employees',\n"," 'discover',\n"," 'decision',\n"," 'dance',\n"," 'china',\n"," 'brings',\n"," 'breaks',\n"," '2017',\n"," 'wearing',\n"," 'starting',\n"," 'russian',\n"," 'possible',\n"," 'opens',\n"," 'mass',\n"," 'major',\n"," 'iowa',\n"," 'hoping',\n"," 'bus',\n"," 'anniversary',\n"," 'am',\n"," 'almost',\n"," 'worst',\n"," 'worried',\n"," 'weird',\n"," 'weight',\n"," 'wars',\n"," 'wait',\n"," 'voice',\n"," 'vacation',\n"," 'university',\n"," 'teens',\n"," 'system',\n"," 'spends',\n"," 'sign',\n"," 'seen',\n"," 'rubio',\n"," 'risk',\n"," 'reform',\n"," 'policy',\n"," 'oscar',\n"," 'oil',\n"," 'industry',\n"," 'important',\n"," 'huffpost',\n"," 'green',\n"," 'google',\n"," 'fear',\n"," 'executive',\n"," 'eat',\n"," 'chance',\n"," 'billion',\n"," 'apple',\n"," 'wishes',\n"," 'whether',\n"," 'true',\n"," 'suicide',\n"," 'reveal',\n"," 'playing',\n"," 'planned',\n"," 'park',\n"," 'murder',\n"," 'moore',\n"," 'killing',\n"," 'happened',\n"," 'colbert',\n"," 'close',\n"," 'album',\n"," 'across',\n"," 'worse',\n"," 'went',\n"," 'voter',\n"," 'try',\n"," 'trans',\n"," 'tour',\n"," 'throws',\n"," 'success',\n"," 'struggling',\n"," 'steve',\n"," 'simple',\n"," 'role',\n"," 'results',\n"," 'reportedly',\n"," 'proud',\n"," 'prince',\n"," 'planet',\n"," 'nyc',\n"," 'moving',\n"," 'magazine',\n"," 'longer',\n"," 'jobs',\n"," 'iraq',\n"," 'hurricane',\n"," 'hilarious',\n"," 'hate',\n"," 'halloween',\n"," 'epa',\n"," 'economy',\n"," 'culture',\n"," 'center',\n"," 'building',\n"," 'artist',\n"," 'amazing',\n"," 'allegations',\n"," 'adds',\n"," 'act',\n"," '2014',\n"," '13',\n"," 'worth',\n"," 'williams',\n"," 'west',\n"," 'table',\n"," 'stand',\n"," 'shares',\n"," 'sales',\n"," 'road',\n"," 'poor',\n"," 'plane',\n"," 'pick',\n"," 'oscars',\n"," 'official',\n"," 'light',\n"," 'leads',\n"," 'keeps',\n"," 'jr',\n"," 'governor',\n"," 'given',\n"," 'general',\n"," 'fuck',\n"," 'finding',\n"," 'final',\n"," 'ferguson',\n"," 'explains',\n"," 'desperate',\n"," 'coworker',\n"," 'chicken',\n"," 'card',\n"," 'bowl',\n"," 'boss',\n"," 'begins',\n"," 'audience',\n"," 'amazon',\n"," 'al',\n"," 'abuse',\n"," '100',\n"," 'weeks',\n"," 'uses',\n"," 'users',\n"," 'truth',\n"," 'transgender',\n"," 'store',\n"," 'sons',\n"," 'snl',\n"," 'side',\n"," 'sean',\n"," 'remember',\n"," 'question',\n"," 'problems',\n"," 'press',\n"," 'performance',\n"," 'peace',\n"," 'nfl',\n"," 'netflix',\n"," 'names',\n"," 'male',\n"," 'lose',\n"," 'lead',\n"," 'key',\n"," 'hopes',\n"," 'holding',\n"," 'harry',\n"," 'hand',\n"," 'force',\n"," 'fighting',\n"," 'far',\n"," 'door',\n"," 'documentary',\n"," 'defense',\n"," 'dark',\n"," 'dads',\n"," 'cut',\n"," 'coworkers',\n"," 'coffee',\n"," 'chicago',\n"," 'carolina',\n"," 'board',\n"," 'which',\n"," 'voting',\n"," 'train',\n"," 'suspect',\n"," 'supporters',\n"," 'steps',\n"," 'spring',\n"," 'senators',\n"," 'robert',\n"," 'refugees',\n"," 'racist',\n"," 'protesters',\n"," 'pregnant',\n"," 'officer',\n"," 'number',\n"," 'michelle',\n"," 'leaving',\n"," 'labor',\n"," 'kerry',\n"," 'incredible',\n"," 'healthy',\n"," 'hall',\n"," 'guys',\n"," 'football',\n"," 'families',\n"," 'faces',\n"," 'easy',\n"," 'dying',\n"," 'driving',\n"," 'dogs',\n"," 'dating',\n"," 'church',\n"," 'ben',\n"," 'asking',\n"," 'arrested',\n"," 'apartment',\n"," 'amid',\n"," 'youth',\n"," 'test',\n"," 'telling',\n"," 'teachers',\n"," 'surprise',\n"," 'suggests',\n"," 'st',\n"," 'speak',\n"," 'soon',\n"," 'slams',\n"," 'return',\n"," 'responds',\n"," 'residents',\n"," 'repeal',\n"," 'push',\n"," 'prevent',\n"," 'picture',\n"," 'perfectly',\n"," 'opening',\n"," 'nice',\n"," 'learn',\n"," 'hour',\n"," 'hotel',\n"," 'homeless',\n"," 'hero',\n"," 'hear',\n"," 'hasnt',\n"," 'funding',\n"," 'feeling',\n"," 'energy',\n"," 'emotional',\n"," 'dreams',\n"," 'demands',\n"," 'demand',\n"," ...]"]},"metadata":{},"execution_count":6}]},{"cell_type":"markdown","source":["#### **Problem #1.1.2: Build the model**\n","\n","\n","Complete the code below to build a model with the following layers.\n","\n","An `Embedding` layer such that:\n","* The vocabulary contains 2000 tokens.\n","* The input length corresponds to the output of the vectorization layer.\n","* The number of outputs per input is 128.\n","\n","<br>\n","\n","Hidden layers:\n","\n","* A  `Flatten` layer.\n","* A  `Dense` layer with 64 units (hidden states).\n","\n","<br>\n","\n","An output `Dense` layer. You can use activation `softmax`."],"metadata":{"id":"_ULrKEoKiKE6"}},{"cell_type":"code","source":["model = Sequential()\n","\n","# Input, Vectorization, and Embedding Layers\n","model.add(Input(shape=(1,), dtype=tf.string))\n","model.add(vectorize_layer)\n","model.add(Embedding(# COMPLETE THIS LINE\n","\n","# Hidden Layers\n","model.add(# COMPLETE THIS LINE\n","\n","# Output Layer\n","model.add(# COMPLETE THIS LINE"],"metadata":{"id":"qliSJAYCiKE6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"error","timestamp":1678876711038,"user_tz":-60,"elapsed":10,"user":{"displayName":"Eranda Bregasi","userId":"06402694060653755536"}},"outputId":"313f519a-84a2-4a9c-8a46-251b1a8defac"},"execution_count":null,"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"ignored","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-15-6c51cf3a7eaf>\"\u001b[0;36m, line \u001b[0;32m12\u001b[0m\n\u001b[0;31m    model.add(# COMPLETE THIS LINE\u001b[0m\n\u001b[0m                                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"]}]},{"cell_type":"markdown","source":["##### **Solution**\n"],"metadata":{"id":"nu7H7YndiKE6"}},{"cell_type":"code","source":["#USING DENSE\n","model = Sequential()\n","\n","# Input, Vectorization, and Embedding Layers\n","model.add(Input(shape=(1,), dtype=tf.string))\n","model.add(vectorize_layer)\n","model.add(Embedding(input_dim = 2000, output_dim = 128, input_length = 20))\n","\n","# Hidden Layers\n","model.add(Flatten()) # Add a Flatten layer before the Dense layer\n","model.add(Dense(64, activation='relu'))\n","\n","# Output Layer\n","model.add(Dense(2, activation = 'softmax'))\n"],"metadata":{"id":"HVVNfWGGzlKO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Problem #1.1.3: Compile and fit the model**\n","\n","\n","Using standard parameters for classification, compile and train this neural network using:\n","* A learning rate of 0.01.\n","* A batch size of 200.\n","* 5 epochs."],"metadata":{"id":"RMhg0mcliKE6"}},{"cell_type":"code","source":["opt = Adam(learning_rate = # COMPLETE THIS LINE\n","model.compile(optimizer = opt, loss = # COMPLETE THIS LINE\n","\n","model.fit(# COMPLETE THIS LINE"],"metadata":{"id":"GIWQr3UFiKE6","colab":{"base_uri":"https://localhost:8080/","height":135},"executionInfo":{"status":"error","timestamp":1679292204185,"user_tz":-60,"elapsed":382,"user":{"displayName":"Eranda Bregasi","userId":"06402694060653755536"}},"outputId":"47dbe164-3533-49bc-971f-c084b76787e3"},"execution_count":null,"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"ignored","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-8-e3a99cb8abb9>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    model.fit(# COMPLETE THIS LINE\u001b[0m\n\u001b[0m                                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"]}]},{"cell_type":"markdown","source":["##### **Solution**\n"],"metadata":{"id":"dlJj2eKsiKE6"}},{"cell_type":"code","source":["opt = Adam(learning_rate = 0.01)\n","model.compile(optimizer = opt, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n","\n","model.fit(x_train, y_train, epochs = 5, batch_size = 200)"],"metadata":{"id":"tzW1Uc4QiKE7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679292217053,"user_tz":-60,"elapsed":11017,"user":{"displayName":"Eranda Bregasi","userId":"06402694060653755536"}},"outputId":"d5d79d34-c1a9-4a67-d82b-c9f5098ed098"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/5\n","107/107 [==============================] - 4s 5ms/step - loss: 0.4098 - accuracy: 0.8054\n","Epoch 2/5\n","107/107 [==============================] - 1s 5ms/step - loss: 0.2373 - accuracy: 0.8963\n","Epoch 3/5\n","107/107 [==============================] - 1s 5ms/step - loss: 0.1545 - accuracy: 0.9374\n","Epoch 4/5\n","107/107 [==============================] - 1s 5ms/step - loss: 0.1054 - accuracy: 0.9569\n","Epoch 5/5\n","107/107 [==============================] - 1s 5ms/step - loss: 0.0867 - accuracy: 0.9653\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f7956ec8910>"]},"metadata":{},"execution_count":9}]},{"cell_type":"markdown","source":["#### **Problem #1.1.4: Evaluate the model**\n","\n","\n","Now, evaluate the model for both the training and test sets."],"metadata":{"id":"rZDdIc2LiKE7"}},{"cell_type":"code","source":[],"metadata":{"id":"rtdeiWm7iKE7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### **Solution**\n"],"metadata":{"id":"qA1rlfoKiKE7"}},{"cell_type":"code","source":["model.evaluate(x_train, y_train)\n","model.evaluate(x_test, y_test)"],"metadata":{"id":"Tno6b9hUiKE7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1678876735013,"user_tz":-60,"elapsed":2594,"user":{"displayName":"Eranda Bregasi","userId":"06402694060653755536"}},"outputId":"6414000c-595c-4871-e787-588e11dab5e4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["668/668 [==============================] - 2s 3ms/step - loss: 0.0503 - accuracy: 0.9815\n","167/167 [==============================] - 0s 3ms/step - loss: 0.8168 - accuracy: 0.8280\n"]},{"output_type":"execute_result","data":{"text/plain":["[0.8168100118637085, 0.8279670476913452]"]},"metadata":{},"execution_count":19}]},{"cell_type":"markdown","source":["---\n","### **Part 1.2: Convolutional Neural Nets**\n","---\n","\n","In this section, you will approach the same problem using the `Conv1D` and `MaxPooling1D` layers provided by keras."],"metadata":{"id":"3D8iKE2U0eZx"}},{"cell_type":"markdown","source":["#### **Problem #1.2.1: Build a CNN**\n","\n","\n","Complete the code below to build, *but not train*, a new CNN model. Specifically, create a model identical to the ones above except with hidden layers as follows:\n","\n","* A `Conv1D` layer with `filters = 1`, `kernel_size = 4`, and `activation = 'relu'`.\n","* A `MaxPooling1D` layer with `pool_size = 2`."],"metadata":{"id":"sKbPF8Z9LAWX"}},{"cell_type":"code","source":["model = Sequential()\n","\n","# Input, Vectorization, and Embedding Layers\n","# COMPLETE THIS CODE\n","\n","\n","# Hidden Layers\n","# COMPLETE THIS CODE\n","\n","\n","# Output Layer\n","# COMPLETE THIS CODE"],"metadata":{"id":"S4_oXj-uLAWY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### **Solution**\n"],"metadata":{"id":"m9cEoLkqLAWY"}},{"cell_type":"code","source":["model = Sequential()\n","\n","# Input, Vectorization, and Embedding Layers\n","model.add(Input(shape=(1,), dtype=tf.string))\n","model.add(vectorize_layer)\n","model.add(Embedding(input_dim = 2000, output_dim = 128, input_length = 20))\n","\n","# Hidden Layers\n","model.add(Conv1D(filters=1, kernel_size=4, activation='relu'))\n","model.add(MaxPooling1D(pool_size=2))\n","model.add(Flatten())\n","\n","# Output Layer\n","model.add(Dense(2, activation = 'softmax'))"],"metadata":{"id":"dCytiF04LAWY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Problem #1.2.2: Examine the CNN's structure**\n","\n","\n","Now, let's look at the structure of this neural network.\n","\n","**Run the code below to print the input and output shapes of each layer.**"],"metadata":{"id":"QFEo-5xuVk2J"}},{"cell_type":"code","source":["for layer in model.layers:\n","  print(str(layer.input_shape) + \" -> \" + str(layer.output_shape))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MtSHQoz0Vv2t","executionInfo":{"status":"ok","timestamp":1678876740605,"user_tz":-60,"elapsed":3,"user":{"displayName":"Eranda Bregasi","userId":"06402694060653755536"}},"outputId":"11118923-1938-41cb-9aa0-c00c3b8ed25c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(None, 1) -> (None, 20)\n","(None, 20) -> (None, 20, 128)\n","(None, 20, 128) -> (None, 17, 1)\n","(None, 17, 1) -> (None, 8, 1)\n","(None, 8, 1) -> (None, 8)\n","(None, 8) -> (None, 2)\n"]}]},{"cell_type":"markdown","source":["#### **Problem #1.2.3: Train and Evaluate the CNN**\n","\n","\n","Now, complete the code below to train and evaluate this model."],"metadata":{"id":"ZjQMsF11VyHq"}},{"cell_type":"code","source":["# Fitting\n","opt = Adam(learning_rate = 0.001)\n","model.compile(optimizer = opt, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n","model.fit(x_train, y_train, epochs = 5, batch_size = 256)\n","\n","\n","# Evaluating\n","print(\"\\n\\n\\n\")\n","model.evaluate(# COMPLETE THIS LINE\n","model.evaluate(# COMPLETE THIS LINE"],"metadata":{"id":"9H-BY04zV5BG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### **Solution**\n"],"metadata":{"id":"aoTwV9nBWMcx"}},{"cell_type":"code","source":["# Fitting\n","opt = Adam(learning_rate = 0.001)\n","model.compile(optimizer = opt, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n","model.fit(x_train, y_train, epochs = 5, batch_size = 256)\n","\n","\n","# Evaluating\n","print(\"\\n\\n\\n\")\n","model.evaluate(x_train, y_train)\n","model.evaluate(x_test, y_test)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xgc4KTFRViIa","executionInfo":{"status":"ok","timestamp":1679292307016,"user_tz":-60,"elapsed":14319,"user":{"displayName":"Eranda Bregasi","userId":"06402694060653755536"}},"outputId":"bc03b694-0f0d-425e-cd2f-6b214fc12005"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/5\n","84/84 [==============================] - 6s 6ms/step - loss: 0.6455 - accuracy: 0.5702\n","Epoch 2/5\n","84/84 [==============================] - 0s 6ms/step - loss: 0.5282 - accuracy: 0.7329\n","Epoch 3/5\n","84/84 [==============================] - 0s 6ms/step - loss: 0.4687 - accuracy: 0.8138\n","Epoch 4/5\n","84/84 [==============================] - 0s 5ms/step - loss: 0.4293 - accuracy: 0.8373\n","Epoch 5/5\n","84/84 [==============================] - 0s 5ms/step - loss: 0.3976 - accuracy: 0.8521\n","\n","\n","\n","\n","668/668 [==============================] - 2s 3ms/step - loss: 0.3698 - accuracy: 0.8712\n","167/167 [==============================] - 1s 3ms/step - loss: 0.4701 - accuracy: 0.7924\n"]},{"output_type":"execute_result","data":{"text/plain":["[0.47007453441619873, 0.7923998236656189]"]},"metadata":{},"execution_count":11}]},{"cell_type":"markdown","source":["#### **Problem #1.2.4: Adjust the model**\n","\n","\n","Complete the code below to train a new CNN model. Specifically, create a model identical to the ones above except with hidden layers as follows:\n","\n","* A `Conv1D` layer with `filters = 64`, `kernel_size = 3`, and `activation = 'relu'`.\n","* A `MaxPooling1D` layer with `pool_size = 2`."],"metadata":{"id":"aCfKk9hkPB2a"}},{"cell_type":"code","source":["model = Sequential()\n","\n","# Input, Vectorization, and Embedding Layers\n","# COMPLETE THIS CODE\n","\n","\n","# Hidden Layers\n","# COMPLETE THIS CODE\n","\n","\n","# Output Layer\n","# COMPLETE THIS CODE\n","\n","\n","# Printing Structure\n","for layer in model.layers:\n","  print(str(layer.input_shape) + \" -> \" + str(layer.output_shape))\n","print(\"\\n\\n\\n\")\n","\n","\n","# Fitting\n","# COMPLETE THIS CODE\n","\n","\n","# Evaluating\n","print(\"\\n\\n\\n\")\n","# COMPLETE THIS CODE"],"metadata":{"id":"G9LN8miLPB2k"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### **Solution**\n"],"metadata":{"id":"j_1A-7HGPB2k"}},{"cell_type":"code","source":["model = Sequential()\n","\n","# Input, Vectorization, and Embedding Layers\n","model.add(Input(shape=(1,), dtype=tf.string))\n","model.add(vectorize_layer)\n","model.add(Embedding(input_dim = 2000, output_dim = 128, input_length = 20))\n","\n","# Hidden Layers\n","model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n","model.add(MaxPooling1D(pool_size=2))\n","model.add(Flatten())\n","\n","# Output Layer\n","model.add(Dense(2, activation = 'softmax'))\n","\n","\n","# Printing Structure\n","for layer in model.layers:\n","  print(str(layer.input_shape) + \" -> \" + str(layer.output_shape))\n","print(\"\\n\\n\\n\")\n","\n","# Fitting\n","opt = Adam(learning_rate = 0.001)\n","model.compile(optimizer = opt, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n","model.fit(x_train, y_train, epochs = 5, batch_size = 256)\n","\n","\n","# Evaluating\n","print(\"\\n\\n\\n\")\n","model.evaluate(x_train, y_train)\n","model.evaluate(x_test, y_test)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"outputId":"48d62601-833b-4100-bc09-539197213282","executionInfo":{"status":"ok","timestamp":1679292316342,"user_tz":-60,"elapsed":9330,"user":{"displayName":"Eranda Bregasi","userId":"06402694060653755536"}},"id":"wtc5lcIxPB2l"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(None, 1) -> (None, 20)\n","(None, 20) -> (None, 20, 128)\n","(None, 20, 128) -> (None, 18, 64)\n","(None, 18, 64) -> (None, 9, 64)\n","(None, 9, 64) -> (None, 576)\n","(None, 576) -> (None, 2)\n","\n","\n","\n","\n","Epoch 1/5\n","84/84 [==============================] - 2s 13ms/step - loss: 0.5451 - accuracy: 0.7115\n","Epoch 2/5\n","84/84 [==============================] - 1s 12ms/step - loss: 0.3149 - accuracy: 0.8647\n","Epoch 3/5\n","84/84 [==============================] - 1s 8ms/step - loss: 0.2571 - accuracy: 0.8928\n","Epoch 4/5\n","84/84 [==============================] - 1s 6ms/step - loss: 0.2161 - accuracy: 0.9144\n","Epoch 5/5\n","84/84 [==============================] - 1s 6ms/step - loss: 0.1827 - accuracy: 0.9296\n","\n","\n","\n","\n","668/668 [==============================] - 2s 3ms/step - loss: 0.1402 - accuracy: 0.9548\n","167/167 [==============================] - 1s 3ms/step - loss: 0.3739 - accuracy: 0.8456\n"]},{"output_type":"execute_result","data":{"text/plain":["[0.37390926480293274, 0.8455634713172913]"]},"metadata":{},"execution_count":12}]},{"cell_type":"markdown","source":["#### **Problem #1.2.5: Improve the model**\n","---\n","\n","You are likely seeing that this last model performs better on the test set than most others you have seen today. It will be challenging to beat this; however, see if you can improve the model any more by adjusting any parameters including:\n","\n","* **Number of filters**: Can we get away with fewer filters? Should we add more filters?\n","* **Kernel size**: What happens when we change this more significantly?\n","* **Pool size**: Should we be pooling more inputs together or fewer?\n","* **Dense layers**: Would adding any dense layers after the convolved results are pooled and flatten help?\n","* **Number of convolutional and pooling layers**: If you're careful about the input and output shapes, it is possible to stack multiple convolutional and pooling layers.\n","* **Training parameters**: Would it help to adjust the learning rate, number of epochs, or batch size?"],"metadata":{"id":"2unPUULPWy_T"}},{"cell_type":"code","source":["model = Sequential()\n","\n","# Input, Vectorization, and Embedding Layers\n","# COMPLETE THIS CODE\n","\n","\n","# Hidden Layers\n","# COMPLETE THIS CODE\n","\n","\n","# Output Layer\n","# COMPLETE THIS CODE\n","\n","\n","# Fitting\n","# COMPLETE THIS CODE\n","\n","\n","# Evaluating\n","print(\"\\n\\n\\n\")\n","# COMPLETE THIS CODE"],"metadata":{"id":"UAbzdj1dWy_c"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### **Solution**\n"],"metadata":{"id":"1qSdafKJWy_c"}},{"cell_type":"code","source":["model = Sequential()\n","\n","# Input, Vectorization, and Embedding Layers\n","model.add(Input(shape=(1,), dtype=tf.string))\n","model.add(vectorize_layer)\n","model.add(Embedding(input_dim = 2000, output_dim = 128, input_length = 20))\n","\n","# Hidden Layers\n","model.add(Conv1D(filters=9, kernel_size=3, activation='relu'))\n","model.add(MaxPooling1D(pool_size=2))\n","\n","#model.add(Flatten())\n","model.add(GRU(64))\n","\n","model.add(Dense(16, activation = 'relu'))\n","\n","\n","# Output Layer\n","model.add(Dense(2, activation = 'softmax'))\n","\n","\n","# Printing Structure\n","for layer in model.layers:\n","  print(str(layer.input_shape) + \" -> \" + str(layer.output_shape))\n","print(\"\\n\\n\\n\")\n","\n","\n","# Fitting\n","opt = Adam(learning_rate = 0.001)\n","model.compile(optimizer = opt, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n","model.fit(x_train, y_train, epochs = 5, batch_size = 256)\n","\n","\n","# Evaluating\n","print(\"\\n\\n\\n\")\n","model.evaluate(x_train, y_train)\n","model.evaluate(x_test, y_test)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"outputId":"8b204452-d664-4c9e-e836-0aad7487b14f","executionInfo":{"status":"ok","timestamp":1679292328004,"user_tz":-60,"elapsed":11666,"user":{"displayName":"Eranda Bregasi","userId":"06402694060653755536"}},"id":"ii4G9reFWy_c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(None, 1) -> (None, 20)\n","(None, 20) -> (None, 20, 128)\n","(None, 20, 128) -> (None, 18, 9)\n","(None, 18, 9) -> (None, 9, 9)\n","(None, 9, 9) -> (None, 64)\n","(None, 64) -> (None, 16)\n","(None, 16) -> (None, 2)\n","\n","\n","\n","\n","Epoch 1/5\n","84/84 [==============================] - 4s 11ms/step - loss: 0.5805 - accuracy: 0.6659\n","Epoch 2/5\n","84/84 [==============================] - 1s 11ms/step - loss: 0.3402 - accuracy: 0.8522\n","Epoch 3/5\n","84/84 [==============================] - 1s 9ms/step - loss: 0.2755 - accuracy: 0.8871\n","Epoch 4/5\n","84/84 [==============================] - 1s 7ms/step - loss: 0.2239 - accuracy: 0.9117\n","Epoch 5/5\n","84/84 [==============================] - 1s 7ms/step - loss: 0.1708 - accuracy: 0.9366\n","\n","\n","\n","\n","668/668 [==============================] - 3s 4ms/step - loss: 0.1080 - accuracy: 0.9691\n","167/167 [==============================] - 1s 4ms/step - loss: 0.4270 - accuracy: 0.8360\n"]},{"output_type":"execute_result","data":{"text/plain":["[0.4269696772098541, 0.8360164761543274]"]},"metadata":{},"execution_count":13}]},{"cell_type":"markdown","metadata":{"id":"7dzC09dLlEhm"},"source":["---\n","#End of notebook\n","\n","© 2024 The Coding School, All rights reserved"]}],"metadata":{"colab":{"provenance":[{"file_id":"1VbvC84SrV5ZOKxogfQXvwey0-q3TipJY","timestamp":1678175654957}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"nbformat":4,"nbformat_minor":0}