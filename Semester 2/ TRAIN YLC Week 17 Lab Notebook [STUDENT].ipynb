{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1eXLCLO1Q16EV272IXUcJpEEtC7c-qMYU","timestamp":1681312728463},{"file_id":"1-p3DhBN3Clv6m9KnAlelA_1IbmE4g8yT","timestamp":1681307059530}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Lab 17: Training Neural Networks**\n","---\n","\n","### **Description**\n","In today's lab you'll learn how to train a neural network and you'll experiment with different parameters for fine-tuning deep learning models.\n","\n","\n","\n","<br>\n","\n","### **Structure**\n","**Part 1**: [Review](#p1)\n","\n","**Part 2**: [The Wine Dataset](#p2)\n","\n","**Part 3**: [Hyperparameter Tuning](#p3)\n","\n","\n","\n","\n","\n","\n","<br>\n","\n","### **Learning Objectives**\n","By the end of this lab, you will:\n","* Recognize how to train and evaluate a Neural Network.\n","* Recognize how to do hyperparameter tuning of learning rate, activation functions, and model architecture (layers and neurons).\n","\n","<br>\n","\n","\n","### **Resources**:\n","* [Deep Learning with keras](https://docs.google.com/document/d/1WCV2ok7dwPWCid5vdOImknCAJS2te5aQ8yRp6J5Clac/edit?usp=sharing)\n","\n","<br>\n","\n","**Before starting, run the code below to import all necessary functions and libraries.**"],"metadata":{"id":"mzyUrRqcU0fd"}},{"cell_type":"code","source":["from keras.models import Sequential\n","from keras.layers import *\n","!pip install --quiet keras_visualizer\n","from keras_visualizer import visualizer\n","from IPython.display import Image\n","\n","from keras.optimizers import Adam, SGD\n","from keras.utils import to_categorical\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","\n","from sklearn.model_selection import train_test_split, cross_val_score\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.datasets import load_breast_cancer\n","from sklearn.linear_model import LinearRegression, LogisticRegression\n","from sklearn.metrics import mean_squared_error, accuracy_score\n","from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"],"metadata":{"id":"tRNgYwf5Dobg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<a name=\"p1\"></a>\n","\n","---\n","## **Part 1: Review**\n","---\n"],"metadata":{"id":"KElX31obQLfZ"}},{"cell_type":"markdown","metadata":{"id":"r02HhDt5CgUy"},"source":["#### **Problem #1.1**\n","\n","Create and visualize a new model with three hidden layers.\n","\n","* The input layer should have `10` neurons\n","\n","* The first hidden layer should have `6` neurons with no activation.\n","\n","* The second hidden layer should have `4` neurons with ReLU activation function.\n","\n","* The third hidden layer should have `12` neurons with ReLU activation.\n","\n","* The output layer should have `4` neurons."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SeIB6lb2CgUy"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"qvr43yG7QfNN"},"source":["#### **Problem #1.2**\n","\n","Create and visualize a new model with three hidden layers.\n","\n","* The input layer should have `8` neurons\n","\n","* The first hidden layer should have `5` neurons with Sigmoid activation.\n","\n","* The second hidden layer should have `8` neurons with ReLU activation function.\n","\n","* The third hidden layer should have `7` neurons with no activation.\n","\n","* The output layer should have `2` neurons."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GyzJsUDgQfNc"},"outputs":[],"source":[]},{"cell_type":"markdown","source":["---\n","\n","<center>\n","\n","#### **Wait for Your Instructor to Continue**\n","\n","---"],"metadata":{"id":"XCjfwEA1QvnG"}},{"cell_type":"markdown","source":["\n","<a name=\"p2\"></a>\n","\n","---\n","## **Part 2: The Breast Cancer Dataset**\n","---\n","\n","The Breast Cancer Wisconsin (Diagnostic) dataset is a widely used dataset in machine learning for binary classification tasks. It contains features computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. The dataset includes 569 instances, each with 30 numeric, real-valued features. These features describe characteristics of the cell nuclei present in the images, such as texture, radius, perimeter, area, smoothness, compactness, concavity, concave points, symmetry, and fractal dimension.\n","\n","The target variable in this dataset is binary, indicating whether the breast mass is malignant or benign. This dataset is particularly significant in medical diagnostic research and machine learning due to its real-world implications in breast cancer diagnosis. It's an excellent resource for teaching machine learning techniques, especially in the context of binary classification problems.\n","\n","**You will use deep learning for classification to predict this variable**:\n","12. `target` (binary value 0 or 1)\n","\n"],"metadata":{"id":"B9HniB0uu8Cr"}},{"cell_type":"markdown","source":["#### **Step #1: Import and and split the data into training/testing**\n","\n","\n","**This is completed for you. Just run the code below!**"],"metadata":{"id":"YFYnYOOQvJqW"}},{"cell_type":"code","source":["data = load_breast_cancer()\n","X = data['data']\n","y = data['target']\n","\n","# Split the data into training and validation sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"],"metadata":{"id":"WPr5rCaru_Ss"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Step #2: Determine the dimension of the data**\n","\n"],"metadata":{"id":"N9PCCGMhWDGI"}},{"cell_type":"code","source":[],"metadata":{"id":"hRFIIRznvSCl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Step #3-6: Building the model**\n","\n","\n","Build a NN such that it has:\n","* The correct number of input neurons (one for each feature).\n","* No hidden layers.\n","* Two output neurons with the `'linear'` activation function.\n","\n","Don't forget to visualize the NN."],"metadata":{"id":"Gm54RjNhWXrR"}},{"cell_type":"code","source":[],"metadata":{"id":"8WX_aGLaX-JY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Step #7: Fit the model**\n","\n","Use the following parameters:\n","\n","* `mse` loss function\n","* accuracy metric\n","* 10 epochs\n","* 0.1 learning rate\n","\n","*Most of the code is given to you.*"],"metadata":{"id":"5amqSJDgYIFU"}},{"cell_type":"code","source":["# Compile the model\n","opt = Adam(# COMPLETE THIS LINE\n","model_nn.compile(# COMPLETE THIS LINE\n","\n","# Fit the model on the training data\n","history = model_nn.fit(# COMPLETE THIS LINE"],"metadata":{"id":"r5xUcvXUYbom"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Step #8: Evaluate the model**\n","\n"],"metadata":{"id":"qrLpLzXCIWIf"}},{"cell_type":"code","source":["print('MSE: ' + str(model_nn.evaluate(# COMPLETE THIS LINE\n"],"metadata":{"id":"VZv_z6q_xB5h"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5KaM7g0s8m_j"},"source":["<a name=\"p3\"></a>\n","\n","---\n","## **Part 3: Hyperparameter Tuning**\n","---\n","\n","In this section, we will see how tuning hyperparameters can affect the performance of a Neural Network."]},{"cell_type":"markdown","metadata":{"id":"r2waalaIpxBz"},"source":["### **Problem #3.1**\n","\n","Modify the code below to find the ideal `learning_rate` parameter given everything else is fixed. Consider values such as: `10`, `1`, `0.1`, `0.01`, `0.001`, and `0.0001`."]},{"cell_type":"code","source":["model_nn = Sequential()\n","model_nn.add(Input(11))\n","model_nn.add(Dense(8, activation='relu'))\n","model_nn.add(Dense(1, activation='linear'))\n","\n","# Compile the model\n","opt = Adam(learning_rate = 0.1)\n","model_nn.compile(optimizer = opt, loss = 'mse', metrics = ['mse'])\n","\n","# Fit the model on the training data\n","history = model_nn.fit(X_train, y_train, epochs=10, batch_size = 64)"],"metadata":{"id":"B-GA895WyCr2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fSp9j4Pn-ZDt"},"source":["### **Problem #3.2**\n","\n","To reach the full potential of smaller learning rates, we often need to compensate by running more epochs. So, modify the code below to increase the `epochs` parameter to `100` and find the ideal `learning_rate` parameter given everything else is fixed. Consider values such as: `10`, `1`, `0.1`, `0.01`, `0.001`, and `0.0001`."]},{"cell_type":"code","source":["model_nn = Sequential()\n","model_nn.add(Input(11))\n","model_nn.add(Dense(8, activation='relu'))\n","model_nn.add(Dense(1, activation='linear'))\n","\n","# Compile the model\n","opt = Adam(learning_rate = 0.1)\n","model_nn.compile(optimizer = opt, loss = 'mse', metrics = ['mse'])\n","\n","# Fit the model on the training data\n","history = model_nn.fit(X_train, y_train, epochs=10, batch_size = 64)"],"metadata":{"id":"sCRTeVWJ-ZD6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GzHIdT4P_1cw"},"source":["### **Problem #3.3**\n","\n","The last main hyperparameter for us to consider is `batch_size`, which is how many data points we use to make a single update to the weights and parameters. Modify the code below to find the ideal `batch_size` given everything else is fixed. Consider values such as: `1`, `32`, `64`, `256`, and `len(X_train)`.\n","\n","<br>\n","\n","**NOTE**: Each epoch involves looking at *all* of the data points. In other words, **size of training data = batch_size * number of updates per epoch**. So, increasing `batch_size`, decreases the number of updates, which in turn speeds up each epoch. As such, pay attention to the amount of time it takes to run through each epoch."]},{"cell_type":"code","source":["model_nn = Sequential()\n","model_nn.add(Input(11))\n","model_nn.add(Dense(8, activation='relu'))\n","model_nn.add(Dense(1, activation='linear'))\n","\n","# Compile the model\n","opt = Adam(learning_rate = 0.01)\n","model_nn.compile(optimizer = opt, loss = 'mse', metrics = ['mse'])\n","\n","# Fit the model on the training data\n","history = model_nn.fit(X_train, y_train, epochs=100, batch_size = 64)"],"metadata":{"id":"6zuFps0I_1c-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702582180524,"user_tz":480,"elapsed":6181,"user":{"displayName":"Angelique Membrido","userId":"04984385250682432912"}},"outputId":"05d63e06-a6e1-45d8-b4e0-7d2a7d4fc5ff"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/100\n","15/15 [==============================] - 1s 3ms/step - loss: 28.7285 - mse: 28.7285\n","Epoch 2/100\n","15/15 [==============================] - 0s 3ms/step - loss: 10.3493 - mse: 10.3493\n","Epoch 3/100\n","15/15 [==============================] - 0s 3ms/step - loss: 6.4206 - mse: 6.4206\n","Epoch 4/100\n","15/15 [==============================] - 0s 2ms/step - loss: 4.7439 - mse: 4.7439\n","Epoch 5/100\n","15/15 [==============================] - 0s 2ms/step - loss: 3.4833 - mse: 3.4833\n","Epoch 6/100\n","15/15 [==============================] - 0s 3ms/step - loss: 2.5702 - mse: 2.5702\n","Epoch 7/100\n","15/15 [==============================] - 0s 2ms/step - loss: 2.0598 - mse: 2.0598\n","Epoch 8/100\n","15/15 [==============================] - 0s 2ms/step - loss: 1.9041 - mse: 1.9041\n","Epoch 9/100\n","15/15 [==============================] - 0s 2ms/step - loss: 1.7148 - mse: 1.7148\n","Epoch 10/100\n","15/15 [==============================] - 0s 2ms/step - loss: 1.6398 - mse: 1.6398\n","Epoch 11/100\n","15/15 [==============================] - 0s 2ms/step - loss: 1.5535 - mse: 1.5535\n","Epoch 12/100\n","15/15 [==============================] - 0s 2ms/step - loss: 1.4524 - mse: 1.4524\n","Epoch 13/100\n","15/15 [==============================] - 0s 3ms/step - loss: 1.4435 - mse: 1.4435\n","Epoch 14/100\n","15/15 [==============================] - 0s 3ms/step - loss: 1.3804 - mse: 1.3804\n","Epoch 15/100\n","15/15 [==============================] - 0s 2ms/step - loss: 1.2720 - mse: 1.2720\n","Epoch 16/100\n","15/15 [==============================] - 0s 2ms/step - loss: 1.2388 - mse: 1.2388\n","Epoch 17/100\n","15/15 [==============================] - 0s 2ms/step - loss: 1.2090 - mse: 1.2090\n","Epoch 18/100\n","15/15 [==============================] - 0s 2ms/step - loss: 1.1953 - mse: 1.1953\n","Epoch 19/100\n","15/15 [==============================] - 0s 3ms/step - loss: 1.1490 - mse: 1.1490\n","Epoch 20/100\n","15/15 [==============================] - 0s 2ms/step - loss: 1.1505 - mse: 1.1505\n","Epoch 21/100\n","15/15 [==============================] - 0s 2ms/step - loss: 1.1309 - mse: 1.1309\n","Epoch 22/100\n","15/15 [==============================] - 0s 2ms/step - loss: 1.1333 - mse: 1.1333\n","Epoch 23/100\n","15/15 [==============================] - 0s 3ms/step - loss: 1.0975 - mse: 1.0975\n","Epoch 24/100\n","15/15 [==============================] - 0s 2ms/step - loss: 1.0959 - mse: 1.0959\n","Epoch 25/100\n","15/15 [==============================] - 0s 2ms/step - loss: 1.1085 - mse: 1.1085\n","Epoch 26/100\n","15/15 [==============================] - 0s 3ms/step - loss: 1.0856 - mse: 1.0856\n","Epoch 27/100\n","15/15 [==============================] - 0s 2ms/step - loss: 1.1018 - mse: 1.1018\n","Epoch 28/100\n","15/15 [==============================] - 0s 2ms/step - loss: 1.0668 - mse: 1.0668\n","Epoch 29/100\n","15/15 [==============================] - 0s 2ms/step - loss: 1.0523 - mse: 1.0523\n","Epoch 30/100\n","15/15 [==============================] - 0s 3ms/step - loss: 1.0374 - mse: 1.0374\n","Epoch 31/100\n","15/15 [==============================] - 0s 3ms/step - loss: 1.0273 - mse: 1.0273\n","Epoch 32/100\n","15/15 [==============================] - 0s 2ms/step - loss: 1.0305 - mse: 1.0305\n","Epoch 33/100\n","15/15 [==============================] - 0s 3ms/step - loss: 1.0384 - mse: 1.0384\n","Epoch 34/100\n","15/15 [==============================] - 0s 3ms/step - loss: 1.0450 - mse: 1.0450\n","Epoch 35/100\n","15/15 [==============================] - 0s 2ms/step - loss: 1.0435 - mse: 1.0435\n","Epoch 36/100\n","15/15 [==============================] - 0s 2ms/step - loss: 1.0822 - mse: 1.0822\n","Epoch 37/100\n","15/15 [==============================] - 0s 2ms/step - loss: 1.1566 - mse: 1.1566\n","Epoch 38/100\n","15/15 [==============================] - 0s 2ms/step - loss: 1.0407 - mse: 1.0407\n","Epoch 39/100\n","15/15 [==============================] - 0s 2ms/step - loss: 1.0132 - mse: 1.0132\n","Epoch 40/100\n","15/15 [==============================] - 0s 3ms/step - loss: 1.0832 - mse: 1.0832\n","Epoch 41/100\n","15/15 [==============================] - 0s 2ms/step - loss: 1.0446 - mse: 1.0446\n","Epoch 42/100\n","15/15 [==============================] - 0s 2ms/step - loss: 1.0075 - mse: 1.0075\n","Epoch 43/100\n","15/15 [==============================] - 0s 3ms/step - loss: 1.0240 - mse: 1.0240\n","Epoch 44/100\n","15/15 [==============================] - 0s 2ms/step - loss: 1.0453 - mse: 1.0453\n","Epoch 45/100\n","15/15 [==============================] - 0s 3ms/step - loss: 1.0287 - mse: 1.0287\n","Epoch 46/100\n","15/15 [==============================] - 0s 2ms/step - loss: 1.0279 - mse: 1.0279\n","Epoch 47/100\n","15/15 [==============================] - 0s 2ms/step - loss: 1.0205 - mse: 1.0205\n","Epoch 48/100\n","15/15 [==============================] - 0s 3ms/step - loss: 0.9985 - mse: 0.9985\n","Epoch 49/100\n","15/15 [==============================] - 0s 2ms/step - loss: 1.0591 - mse: 1.0591\n","Epoch 50/100\n","15/15 [==============================] - 0s 2ms/step - loss: 1.0487 - mse: 1.0487\n","Epoch 51/100\n","15/15 [==============================] - 0s 2ms/step - loss: 1.0432 - mse: 1.0432\n","Epoch 52/100\n","15/15 [==============================] - 0s 2ms/step - loss: 1.0195 - mse: 1.0195\n","Epoch 53/100\n","15/15 [==============================] - 0s 3ms/step - loss: 1.0622 - mse: 1.0622\n","Epoch 54/100\n","15/15 [==============================] - 0s 3ms/step - loss: 1.0548 - mse: 1.0548\n","Epoch 55/100\n","15/15 [==============================] - 0s 2ms/step - loss: 1.0630 - mse: 1.0630\n","Epoch 56/100\n","15/15 [==============================] - 0s 2ms/step - loss: 1.0931 - mse: 1.0931\n","Epoch 57/100\n","15/15 [==============================] - 0s 2ms/step - loss: 1.0239 - mse: 1.0239\n","Epoch 58/100\n","15/15 [==============================] - 0s 3ms/step - loss: 1.0767 - mse: 1.0767\n","Epoch 59/100\n","15/15 [==============================] - 0s 2ms/step - loss: 1.0292 - mse: 1.0292\n","Epoch 60/100\n","15/15 [==============================] - 0s 2ms/step - loss: 1.0069 - mse: 1.0069\n","Epoch 61/100\n","15/15 [==============================] - 0s 2ms/step - loss: 1.1038 - mse: 1.1038\n","Epoch 62/100\n","15/15 [==============================] - 0s 2ms/step - loss: 1.0280 - mse: 1.0280\n","Epoch 63/100\n","15/15 [==============================] - 0s 2ms/step - loss: 0.9922 - mse: 0.9922\n","Epoch 64/100\n","15/15 [==============================] - 0s 2ms/step - loss: 1.0202 - mse: 1.0202\n","Epoch 65/100\n","15/15 [==============================] - 0s 2ms/step - loss: 1.0083 - mse: 1.0083\n","Epoch 66/100\n","15/15 [==============================] - 0s 3ms/step - loss: 1.0082 - mse: 1.0082\n","Epoch 67/100\n","15/15 [==============================] - 0s 2ms/step - loss: 1.0579 - mse: 1.0579\n","Epoch 68/100\n","15/15 [==============================] - 0s 2ms/step - loss: 1.3773 - mse: 1.3773\n","Epoch 69/100\n","15/15 [==============================] - 0s 2ms/step - loss: 1.3981 - mse: 1.3981\n","Epoch 70/100\n","15/15 [==============================] - 0s 2ms/step - loss: 1.2963 - mse: 1.2963\n","Epoch 71/100\n","15/15 [==============================] - 0s 3ms/step - loss: 1.0365 - mse: 1.0365\n","Epoch 72/100\n","15/15 [==============================] - 0s 2ms/step - loss: 1.0975 - mse: 1.0975\n","Epoch 73/100\n","15/15 [==============================] - 0s 2ms/step - loss: 0.9978 - mse: 0.9978\n","Epoch 74/100\n","15/15 [==============================] - 0s 2ms/step - loss: 1.0119 - mse: 1.0119\n","Epoch 75/100\n","15/15 [==============================] - 0s 2ms/step - loss: 1.0081 - mse: 1.0081\n","Epoch 76/100\n","15/15 [==============================] - 0s 2ms/step - loss: 1.0367 - mse: 1.0367\n","Epoch 77/100\n","15/15 [==============================] - 0s 3ms/step - loss: 1.0805 - mse: 1.0805\n","Epoch 78/100\n","15/15 [==============================] - 0s 2ms/step - loss: 1.0671 - mse: 1.0671\n","Epoch 79/100\n","15/15 [==============================] - 0s 2ms/step - loss: 1.0300 - mse: 1.0300\n","Epoch 80/100\n","15/15 [==============================] - 0s 2ms/step - loss: 0.9978 - mse: 0.9978\n","Epoch 81/100\n","15/15 [==============================] - 0s 2ms/step - loss: 1.0033 - mse: 1.0033\n","Epoch 82/100\n","15/15 [==============================] - 0s 2ms/step - loss: 1.0421 - mse: 1.0421\n","Epoch 83/100\n","15/15 [==============================] - 0s 2ms/step - loss: 1.0749 - mse: 1.0749\n","Epoch 84/100\n","15/15 [==============================] - 0s 2ms/step - loss: 0.9927 - mse: 0.9927\n","Epoch 85/100\n","15/15 [==============================] - 0s 2ms/step - loss: 1.0369 - mse: 1.0369\n","Epoch 86/100\n","15/15 [==============================] - 0s 3ms/step - loss: 1.0655 - mse: 1.0655\n","Epoch 87/100\n","15/15 [==============================] - 0s 3ms/step - loss: 0.9889 - mse: 0.9889\n","Epoch 88/100\n","15/15 [==============================] - 0s 2ms/step - loss: 0.9826 - mse: 0.9826\n","Epoch 89/100\n","15/15 [==============================] - 0s 2ms/step - loss: 1.0024 - mse: 1.0024\n","Epoch 90/100\n","15/15 [==============================] - 0s 2ms/step - loss: 1.2332 - mse: 1.2332\n","Epoch 91/100\n","15/15 [==============================] - 0s 2ms/step - loss: 1.3280 - mse: 1.3280\n","Epoch 92/100\n","15/15 [==============================] - 0s 2ms/step - loss: 1.1663 - mse: 1.1663\n","Epoch 93/100\n","15/15 [==============================] - 0s 3ms/step - loss: 1.0422 - mse: 1.0422\n","Epoch 94/100\n","15/15 [==============================] - 0s 2ms/step - loss: 0.9905 - mse: 0.9905\n","Epoch 95/100\n","15/15 [==============================] - 0s 2ms/step - loss: 0.9908 - mse: 0.9908\n","Epoch 96/100\n","15/15 [==============================] - 0s 2ms/step - loss: 1.0029 - mse: 1.0029\n","Epoch 97/100\n","15/15 [==============================] - 0s 2ms/step - loss: 1.0887 - mse: 1.0887\n","Epoch 98/100\n","15/15 [==============================] - 0s 2ms/step - loss: 1.0540 - mse: 1.0540\n","Epoch 99/100\n","15/15 [==============================] - 0s 2ms/step - loss: 0.9897 - mse: 0.9897\n","Epoch 100/100\n","15/15 [==============================] - 0s 3ms/step - loss: 1.0644 - mse: 1.0644\n"]}]},{"cell_type":"markdown","source":["### **Problem #3.4**\n","\n","With so many hyperparameters to tune and potentially deep & complex NNs, the amount of information printed out often becomes less helpful to dig through. It is common practice to visualize the performance of different models for each epoch instead. Furthermore, we can \"mute\" the output of the `fit(...)` function by setting `verbose = False`.\n","\n","<br>\n","\n","**Complete the code below to train and visualize the losses of a NN with the best set of hyperparameters you found above.**\n","\n","\n","Some questions to consider:\n","* Does the loss tend to increase or decrease?\n","* Should we have let the model train for more epochs?\n","* Could we have gotten away with fewer epochs?\n"],"metadata":{"id":"a_J7TGOlvPBG"}},{"cell_type":"code","source":["# BUILD AND TRAIN\n","model_nn = Sequential()\n","model_nn.add(Input(11))\n","model_nn.add(Dense(8, activation='relu'))\n","model_nn.add(Dense(1, activation='linear'))\n","\n","# Compile the model\n","opt = Adam(learning_rate = # COMPLETE THIS LINE\n","model_nn.compile(optimizer = opt, loss = 'mse', metrics = ['mse'])\n","\n","# Fit the model on the training data\n","history = model_nn.fit(X_train, y_train, verbose = False, # COMPLETE THIS LINE\n","\n","\n","# VISUALIZE\n","loss = history.history['loss']\n","plt.plot(loss)\n","\n","plt.title('Loss After Each Epoch', fontsize = 'x-large')\n","plt.xlabel('Epoch', fontsize = 'x-large')\n","plt.ylabel('Loss', fontsize = 'x-large')\n","plt.show()"],"metadata":{"id":"62uQBW52D9mN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","##Â© 2024 The Coding School, All rights reserved"],"metadata":{"id":"21uTwbSQMZnJ"}}]}